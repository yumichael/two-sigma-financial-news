{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "import globals as top_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_M = lambda: pd.read_pickle(the_data/'given/M.pkl')\n",
    "get_N = lambda: pd.read_pickle(the_data/'given/N.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_M:\n",
    "    M = get_M()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### very important ID assignment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_M:\n",
    "    class IdAssign:\n",
    "        class NO_MISSING(metaclass=staticclass): # special filler value because `None` might actually be meaningfully used\n",
    "            pass\n",
    "        def __init__(self, init, missing, name):\n",
    "            self.name = name\n",
    "            self.map = {}\n",
    "            if missing is not __class__.NO_MISSING:\n",
    "                self.map[missing] = -1\n",
    "            i = 0\n",
    "            for x in init:\n",
    "                if x not in self.map and x != missing:\n",
    "                    self.map[x] = i\n",
    "                    i += 1\n",
    "            self.cache = None\n",
    "        def __call__(self, key):\n",
    "            if key not in self.map:\n",
    "                self.map[key] = len(self.map)\n",
    "            return self.map[key]\n",
    "        def __len__(self):\n",
    "            return self.map.__len__()\n",
    "        @property\n",
    "        def series(self):\n",
    "            if self.cache==len(self):\n",
    "                return self._series\n",
    "            # Python 3.?+ guarantees that dict keys and values are itered in same order, and that order is insertion order\n",
    "            self._series = pd.Series(list(self.map.keys()), index=list(self.map.values()), name=self.name)\n",
    "            self._series.index.name = self.name+'Id'\n",
    "            self.cache = len(self)\n",
    "            return self._series\n",
    "\n",
    "    assetCodeSeries = pd.Series(M.assetCode.unique())\n",
    "    assetCodeIdAssign = IdAssign(assetCodeSeries, missing='', name='assetCode')\n",
    "    del assetCodeSeries\n",
    "    assetNameSeries = pd.Series(M.assetName.unique())\n",
    "    assetNameIdAssign = IdAssign(assetNameSeries, missing='Unknown', name='assetName')\n",
    "    del assetNameSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_M:\n",
    "    M['assetCodeId'] = M.assetCode.map(assetCodeIdAssign).astype(int)\n",
    "    M['assetNameId'] = M.assetName.map(assetNameIdAssign).astype(int)\n",
    "    if top_imports.use_N:\n",
    "        N = get_N()\n",
    "        N['assetNameId'] = N.assetName.map(assetNameIdAssign) # set up assetNameIdAssign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_N:\n",
    "    def makeMultimap(fr, to):\n",
    "        ret = {}\n",
    "        for f, t in zip(fr, to):\n",
    "            ret.setdefault(f, set()).add(t)\n",
    "        return ret\n",
    "    assetNameMapCodes = makeMultimap(M.assetNameId, M.assetCodeId)\n",
    "\n",
    "    def makeSinglemap(fr, to):\n",
    "        ret = {}\n",
    "        for f, t in zip(fr, to):\n",
    "            if f in ret:\n",
    "                assert t==ret[f], 'a \"from\" element must be mapped to a unique \"to\" element!'\n",
    "            else:\n",
    "                ret[f] = t\n",
    "        return ret\n",
    "    assetCodeMapName = makeSinglemap(M.assetCodeId, M.assetNameId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_N:\n",
    "    headlineTagSeries = pd.Series(N.headlineTag.unique())\n",
    "    headlineTagIdAssign = IdAssign(headlineTagSeries, missing='', name='headlineTag')\n",
    "    del headlineTagSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_N:\n",
    "    del N;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convenience functions, only for interactive use, no scripting/inside loop please because very unoptimized\n",
    "\n",
    "if top_imports.use_M:\n",
    "    from collections import Iterable\n",
    "\n",
    "    def aId2Code(a):\n",
    "        if hasattr(a, 'map'):\n",
    "            return a.map(assetCodeIdAssign.series)\n",
    "        elif isinstance(a, Iterable) and not isinstance(a, str):\n",
    "            assetCodeMap = assetCodeIdAssign.series.to_dict()\n",
    "            return type(a)(map(lambda x: assetCodeMap[x], a))\n",
    "        else:\n",
    "            return assetCodeIdAssign.series.loc[a]\n",
    "\n",
    "    if top_imports.use_N:\n",
    "        def aId2Name(a):\n",
    "            if hasattr(a, 'map'):\n",
    "                return a.map(assetNameIdAssign.series)\n",
    "            elif isinstance(a, Iterable) and not isinstance(a, str):\n",
    "                assetNameMap = assetNameIdAssign.series.to_dict()\n",
    "                return type(a)(map(lambda x: assetNameMap[x], a))\n",
    "            else:\n",
    "                return assetNameIdAssign.series.loc[a]\n",
    "        def aCodeId2Name(a):\n",
    "            if hasattr(a, 'map'):\n",
    "                return a.map(assetCodeMapName).map(assetNameIdAssign.series)\n",
    "            elif isinstance(a, Iterable) and not isinstance(a, str):\n",
    "                assetNameMap = assetNameIdAssign.series.to_dict()\n",
    "                return type(a)(map(lambda x: assetNameMap[assetCodeMapName[x]], a))\n",
    "            else:\n",
    "                return assetNameIdAssign.series.loc[assetCodeMapName[a]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some data specific helper func/structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringify_columns = lambda f: '[\"'+'\",\"'.join(c for c in f)+'\"]'\n",
    "\n",
    "returns_columns = {\n",
    "    'returnsClosePrevRaw1':'cc', 'returnsOpenPrevRaw1':'oo',\n",
    "    'returnsClosePrevMktres1':'acc', 'returnsOpenPrevMktres1':'aoo',\n",
    "    'returnsClosePrevRaw10':'ccTEN','returnsOpenPrevRaw10':'ooTEN',\n",
    "    'returnsClosePrevMktres10':'accTEN','returnsOpenPrevMktres10':'aooTEN'\n",
    "}\n",
    "columns_for_U = set(returns_columns.values()) | set(['open', 'close', 'volume'])\n",
    "excluded_columns = [\n",
    "    'time','assetCode','assetName','universe','returnsOpenNextMktres10','quarter','y'\n",
    "]\n",
    "exclusion_filter = lambda c: c not in excluded_columns\n",
    "object_columns = ['assetCode', 'assetName']\n",
    "enumeration_columns = ['assetCodeId', 'assetNameId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hard coded constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_start_time = pd.Timestamp('2009-01-01',tz='UTC')\n",
    "lookback = 60\n",
    "shortterm = 21\n",
    "longterm = 250 + lookback*2 #idk it's hard to reason about how much I _really_ need, so this should be pretty safe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting up only the stocks that we have seen in-universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_M:\n",
    "    inUniCount = M[M.time>=train_start_time].groupby('assetCodeId').universe.sum()\n",
    "    stocksInUni = (inUniCount != 0).pipe(lambda x: x.index[x])\n",
    "    assert stocksInUni.is_monotonic\n",
    "    xStocksInUni = set(stocksInUni) #EDITCELL\n",
    "    inUniCountSeries = inUniCount.astype(float) #EDITCELL\n",
    "    stocksAlways = (inUniCount >= 1981).pipe(lambda x: x.index[x])\n",
    "    xStocksAlways = set(stocksAlways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_basic_features(*,M,delete=True):\n",
    "    M['assetCodeId'] = M.assetCode.map(assetCodeIdAssign)#.astype(int) # much more efficient to process everything as float\n",
    "    if delete:\n",
    "        del M['assetCode']\n",
    "    M['assetNameId'] = M.assetName.map(assetNameIdAssign)#.astype(int)\n",
    "    if delete:\n",
    "        del M['assetName']\n",
    "    for orig_col, new_col in returns_columns.items():\n",
    "        M[new_col] = np.log1p(M[orig_col])\n",
    "        if delete:\n",
    "            del M[orig_col]\n",
    "    # time features\n",
    "    M['dayOfYear'] = M.time.dt.dayofyear.astype(float)\n",
    "    M['dayOfWeek'] = M.time.dt.dayofweek.astype(float)\n",
    "\n",
    "if top_imports.use_M:\n",
    "    set_basic_features(M=M,delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_P(*, F, P, vp=True, always=True):\n",
    "    P['quarter'] = P.time.dt.year+(P.time.dt.quarter-1)/4\n",
    "    P['target'] = P.y>0\n",
    "    P['upDown'] = (P.target*2-1)\n",
    "    P['absVal'] = np.abs(P.y)\n",
    "    P['flat_weight'] = P.absVal\n",
    "    if vp:\n",
    "        P['vp1_weight'] = P.absVal*F.vp1/1e9\n",
    "        P['vp5_weight'] = P.absVal*F.vp5/1e9\n",
    "        P['vp10_weight'] = P.absVal*F.vp10/1e9\n",
    "        P['vp20_weight'] = P.absVal*F.vp20/1e9\n",
    "    if always:\n",
    "        P['always_weight'] = P.absVal*F.assetCodeId.isin(xStocksAlways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pairs setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper.bisect import bisect_left as binary_search\n",
    "\n",
    "@njit\n",
    "def index_unique_pairs(A, B, values, repeat):\n",
    "    assert len(A)==len(B)\n",
    "    seen = [0] * len(values)\n",
    "    ans = []\n",
    "    for i, (a, b) in enumerate(zip(A, B)):\n",
    "        ia = binary_search(values, a); ib = binary_search(values, b);\n",
    "        if repeat==-1 or (seen[ia]<repeat and seen[ib]<repeat):\n",
    "            seen[ia] += 1; seen[ib] += 1;\n",
    "            ans.append(i)\n",
    "    return ans\n",
    "\n",
    "def make_unique_pairs(*, Dl, repeat=-1):\n",
    "    if repeat==-1:\n",
    "        return Dl\n",
    "    ii = index_unique_pairs(Dl[0].values, Dl[1].values, sorted(set(Dl[0].values)|set(Dl[1].values)), repeat=repeat)\n",
    "    return Dl.iloc[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_CC_G_Q_from_Dl_F_P_W(*, Dl, F, P, W, lo, hi=None, repeat=-1,\n",
    "                              roll_corr=list(product(['ay1','ay10','ay20'],[10,21,62,250]))):\n",
    "    SEE(f'(pair) len(F.columns) = {len(F.columns)}')\n",
    "    \n",
    "    D = Dl\n",
    "    allTime = pd.Series(P.time.unique())\n",
    "    allTime.index = allTime\n",
    "    \n",
    "    # |begin| subset the pairs list\n",
    "    if hi is None:\n",
    "        hi = (1., 9999)\n",
    "    D = D[(lo[0]<=D.Corr)&(D.Corr<=hi[0])&(lo[1]<=D.Unic)&(D.Unic<=hi[1])]\n",
    "    if isinstance(repeat, int):\n",
    "        D = D.pipe(lambda x: make_unique_pairs(Dl=x, repeat=repeat))\n",
    "        w = (D.Corr.fillna(0)*0 + 1.).values\n",
    "    elif repeat=='prop':\n",
    "        D01 = D\n",
    "        D10 = D[[1,0,'Corr','Unic']]\n",
    "        D2 = pd.concat([D01,D10], axis=0)\n",
    "        aw = D2.groupby(0).Corr.sum()\n",
    "        w = (D.Corr / np.maximum(D[0].map(aw), D[1].map(aw))).values\n",
    "        w /= w.mean()\n",
    "    else:\n",
    "        assert False\n",
    "    hgt = sum(p[-1] for p in D.itertuples())\n",
    "    CC = np.zeros((hgt,2+len(roll_corr)))\n",
    "    G0, G1 = np.zeros((hgt,len(F.columns))), np.zeros((hgt,len(F.columns)))\n",
    "    Q0, Q1 = [], []\n",
    "    # |end| subset the pairs list\n",
    "    \n",
    "    i = 0\n",
    "    SEE(f'(pair) generating [{len(D)}]', end='')\n",
    "    for ii, ((_,a0,a1,corr,unic), weight) in enumerate(zip(D.itertuples(),w)):\n",
    "        print_progress(ii, dot=10, print=SEE)\n",
    "        assert len(weight.shape)==0\n",
    "        in0, in1 = (F.assetCodeId==a0)&(P.universe!=0), (F.assetCodeId==a1)&(P.universe!=0)\n",
    "        A0, A1 = F[in0], F[in1]\n",
    "        B0, B1 = P[in0], P[in1]\n",
    "        inTime = set(B0.time)&set(B1.time)\n",
    "        tm0, tm1 = B0.time.isin(inTime), B1.time.isin(inTime)\n",
    "        A0, A1 = A0[tm0], A1[tm1]\n",
    "        B0, B1 = B0[tm0], B1[tm1]\n",
    "        \n",
    "        tm = allTime.isin(inTime).values\n",
    "        def iter_roll_corr():\n",
    "            for y, r in roll_corr:\n",
    "                yield W[y][a0].rolling(window=r, min_periods=5).corr(W[y][a1])[tm] #TODO hard coded min_periods\n",
    "        unic_ = [np.ones_like(W[next(iter(W))].iloc[:,0][tm])*unic]\n",
    "        weight_ = [np.ones_like(W[next(iter(W))].iloc[:,0][tm])*weight]\n",
    "        C = np.stack(chain(iter_roll_corr(),unic_,weight_), axis=1)\n",
    "        \n",
    "        G0[i:i+unic], G1[i:i+unic] = A0.values, A1.values\n",
    "        Q0.append(B0); Q1.append(B1);\n",
    "        CC[i:i+unic] = C\n",
    "        \n",
    "        i += unic\n",
    "    SEE()\n",
    "    \n",
    "    Fcols = F.columns\n",
    "    del Dl, F, P, W; gc.collect()\n",
    "        \n",
    "    Q0, Q1 = pd.concat(Q0, axis=0), pd.concat(Q1, axis=0)\n",
    "    G0, G1 = pd.DataFrame(G0, index=Q0.index, columns=Fcols), pd.DataFrame(G1, index=Q1.index, columns=Fcols)\n",
    "    CC = pd.DataFrame(CC, columns=[f'__corr__{y}_{r}' for y,r in roll_corr]+['bothInUniCount','weight'])\n",
    "    return CC, (G0,G1), (Q0,Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_FG_PQ_from_CC_G_Q(copy_filter, diff_filter, plus_filter, corr_filter, *, CC, G, Q):\n",
    "    SEE('(pair) putting together...', end=' ')\n",
    "    \n",
    "    weight = CC['weight'].values\n",
    "    \n",
    "    Gcols, Qcols = G[0].columns, Q[0].columns\n",
    "    assert (Gcols==G[1].columns).all() and (Qcols==Q[1].columns).all()\n",
    "    \n",
    "    \n",
    "    cop, dif, pls = list(filter(copy_filter, Gcols)), list(filter(diff_filter, Gcols)), list(filter(plus_filter, Gcols))\n",
    "    G0, G1 = G[0][cop].values, G[1][cop].values\n",
    "    H01 = G[0][dif].values - G[1][dif].values\n",
    "    I01 = G[0][pls].values + G[1][pls].values\n",
    "    CC = CC[list(filter(corr_filter, CC.columns))]\n",
    "    \n",
    "    F01, F10 = np.concatenate([G0,G1,H01,I01,CC.values], axis=1), np.concatenate([G1,G0,-H01,I01,CC.values], axis=1)\n",
    "    FG = np.concatenate([F01, F10], axis=0)\n",
    "    if not np.issubdtype(FG.dtype, np.float):\n",
    "        FG = FG.astype(np.float32)\n",
    "    \n",
    "    FGcols = (['__0__'+c for c in cop] + ['__1__'+c for c in cop] + ['__0-1__'+c for c in dif]\n",
    "              + ['__0+1__'+c for c in pls] + list(CC.columns))\n",
    "    FG = pd.DataFrame(FG, columns=FGcols, copy=False)\n",
    "    \n",
    "    \n",
    "    Q = Q[0].reset_index(drop=True), Q[1].reset_index(drop=True)\n",
    "    assert(Q[0].time==Q[1].time).all()\n",
    "    \n",
    "    #TODO again we don't need universe here\n",
    "    Q01 = pd.DataFrame(dict(time=Q[0].time, y=(Q[0].y-Q[1].y),\n",
    "        **{f'__0__{c}': Q[0][c] for c in ['assetCode','assetCodeId','assetName','assetNameId'] if c in Q[0]},\n",
    "        **{f'__1__{c}': Q[1][c] for c in ['assetCode','assetCodeId','assetName','assetNameId'] if c in Q[1]}))\n",
    "    Q10 = pd.DataFrame(dict(time=Q[1].time, y=(Q[1].y-Q[0].y),\n",
    "        **{f'__0__{c}': Q[1][c] for c in ['assetCode','assetCodeId','assetName','assetNameId'] if c in Q[1]},\n",
    "        **{f'__1__{c}': Q[0][c] for c in ['assetCode','assetCodeId','assetName','assetNameId'] if c in Q[0]}))\n",
    "    \n",
    "    PQ = pd.concat([Q01, Q10], axis=0, ignore_index=True)\n",
    "    PQ['pair_weight'] = np.concatenate([weight, weight], axis=0)\n",
    "    \n",
    "    PQ.sort_values('time', inplace=True)\n",
    "    FG = FG.reindex(index=PQ.index, copy=False)\n",
    "    \n",
    "    FG.reset_index(drop=True, inplace=True)\n",
    "    PQ.reset_index(drop=True, inplace=True)\n",
    "    setup_P(F=None, P=PQ, vp=False, always=False)\n",
    "    \n",
    "    SEE('(pair) done')\n",
    "    return FG, PQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
