{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "import globals as top_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quarter = lambda time: (lambda time: (time.dt.year+(time.dt.quarter-1)/4).astype(np.float32))(pd.Series(time))\n",
    "halfYear = lambda time: (quarter(time)*2).astype(int).values/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_M = lambda: pd.read_pickle(the_data/'given/M.pkl')\n",
    "get_N = lambda: pd.read_pickle(the_data/'given/N.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_M:\n",
    "    M = get_M()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert32(F):\n",
    "    '''(inplace) just converts all columns in dataframe `F` into 32-bit dtypes'''\n",
    "    for c in F.columns:\n",
    "        if np.issubdtype(F[c].dtype, np.float):\n",
    "            if F[c].dtype != np.float32:\n",
    "                F[c] = F[c].astype(np.float32)\n",
    "        elif np.issubdtype(F[c].dtype, np.integer):\n",
    "            if F[c].dtype != np.int32:\n",
    "                F[c] = F[c].astype(np.int32)\n",
    "        else:\n",
    "            assert False, 'dtype other than float or int found in features'\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### very important ID assignment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_M:\n",
    "    class IdAssign:\n",
    "        class NO_MISSING(metaclass=staticclass): # special filler value because `None` might actually be meaningfully used\n",
    "            pass\n",
    "        def __init__(self, init, missing, name):\n",
    "            self.name = name\n",
    "            self.map = {}\n",
    "            if missing is not __class__.NO_MISSING:\n",
    "                self.map[missing] = -1\n",
    "            i = 0\n",
    "            for x in init:\n",
    "                if x not in self.map and x != missing:\n",
    "                    self.map[x] = i\n",
    "                    i += 1\n",
    "            self.cache = None\n",
    "        def __call__(self, key):\n",
    "            if key not in self.map:\n",
    "                self.map[key] = len(self.map)\n",
    "            return self.map[key]\n",
    "        def __len__(self):\n",
    "            return self.map.__len__()\n",
    "        @property\n",
    "        def series(self):\n",
    "            if self.cache==len(self):\n",
    "                return self._series\n",
    "            # Python 3.?+ guarantees that dict keys and values are itered in same order, and that order is insertion order\n",
    "            self._series = pd.Series(list(self.map.keys()), index=list(self.map.values()), name=self.name)\n",
    "            self._series.index.name = self.name+'Id'\n",
    "            self.cache = len(self)\n",
    "            return self._series\n",
    "\n",
    "    assetCodeSeries = pd.Series(M.assetCode.unique())\n",
    "    assetCodeIdAssign = IdAssign(assetCodeSeries, missing='', name='assetCode')\n",
    "    del assetCodeSeries\n",
    "    assetNameSeries = pd.Series(M.assetName.unique())\n",
    "    assetNameIdAssign = IdAssign(assetNameSeries, missing='Unknown', name='assetName')\n",
    "    del assetNameSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_M:\n",
    "    M['assetCodeId'] = M.assetCode.map(assetCodeIdAssign).astype(int)\n",
    "    M['assetNameId'] = M.assetName.map(assetNameIdAssign).astype(int)\n",
    "    if top_imports.use_N:\n",
    "        N = get_N()\n",
    "        N['assetNameId'] = N.assetName.map(assetNameIdAssign) # set up assetNameIdAssign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_N:\n",
    "    def makeMultimap(fr, to):\n",
    "        ret = {}\n",
    "        for f, t in zip(fr, to):\n",
    "            ret.setdefault(f, set()).add(t)\n",
    "        return ret\n",
    "    assetNameMapCodes = makeMultimap(M.assetNameId, M.assetCodeId)\n",
    "\n",
    "    def makeSinglemap(fr, to):\n",
    "        ret = {}\n",
    "        for f, t in zip(fr, to):\n",
    "            if f in ret:\n",
    "                assert t==ret[f], 'a \"from\" element must be mapped to a unique \"to\" element!'\n",
    "            else:\n",
    "                ret[f] = t\n",
    "        return ret\n",
    "    assetCodeMapName = makeSinglemap(M.assetCodeId, M.assetNameId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_N:\n",
    "    headlineTagSeries = pd.Series(N.headlineTag.unique())\n",
    "    headlineTagIdAssign = IdAssign(headlineTagSeries, missing='', name='headlineTag')\n",
    "    del headlineTagSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_N:\n",
    "    del N;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convenience functions, only for interactive use, no scripting/inside loop please because very unoptimized\n",
    "\n",
    "if top_imports.use_M:\n",
    "    from collections import Iterable\n",
    "\n",
    "    def aId2Code(a):\n",
    "        if hasattr(a, 'map'):\n",
    "            return a.map(assetCodeIdAssign.series)\n",
    "        elif isinstance(a, Iterable) and not isinstance(a, str):\n",
    "            assetCodeMap = assetCodeIdAssign.series.to_dict()\n",
    "            return type(a)(map(lambda x: assetCodeMap[x], a))\n",
    "        else:\n",
    "            return assetCodeIdAssign.series.loc[a]\n",
    "\n",
    "    if top_imports.use_N:\n",
    "        def aId2Name(a):\n",
    "            if hasattr(a, 'map'):\n",
    "                return a.map(assetNameIdAssign.series)\n",
    "            elif isinstance(a, Iterable) and not isinstance(a, str):\n",
    "                assetNameMap = assetNameIdAssign.series.to_dict()\n",
    "                return type(a)(map(lambda x: assetNameMap[x], a))\n",
    "            else:\n",
    "                return assetNameIdAssign.series.loc[a]\n",
    "        def aCodeId2Name(a):\n",
    "            if hasattr(a, 'map'):\n",
    "                return a.map(assetCodeMapName).map(assetNameIdAssign.series)\n",
    "            elif isinstance(a, Iterable) and not isinstance(a, str):\n",
    "                assetNameMap = assetNameIdAssign.series.to_dict()\n",
    "                return type(a)(map(lambda x: assetNameMap[assetCodeMapName[x]], a))\n",
    "            else:\n",
    "                return assetNameIdAssign.series.loc[assetCodeMapName[a]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some data specific helper func/structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringify_columns = lambda f: '[\"'+'\",\"'.join(c for c in f)+'\"]'\n",
    "\n",
    "returns_columns = {\n",
    "    'returnsClosePrevRaw1':'cc', 'returnsOpenPrevRaw1':'oo',\n",
    "    'returnsClosePrevMktres1':'acc', 'returnsOpenPrevMktres1':'aoo',\n",
    "    'returnsClosePrevRaw10':'ccTEN','returnsOpenPrevRaw10':'ooTEN',\n",
    "    'returnsClosePrevMktres10':'accTEN','returnsOpenPrevMktres10':'aooTEN'\n",
    "}\n",
    "columns_for_U = set(returns_columns.values()) | set(['open', 'close', 'volume'])\n",
    "excluded_columns = [\n",
    "    'time','assetCode','assetName','universe','returnsOpenNextMktres10','quarter','y'\n",
    "]\n",
    "exclusion_filter = lambda c: c not in excluded_columns\n",
    "object_columns = ['assetCode', 'assetName']\n",
    "enumeration_columns = ['assetCodeId', 'assetNameId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hard coded constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_start_time = pd.Timestamp('2009-01-01',tz='UTC')\n",
    "lookback = 60\n",
    "shortterm = 21\n",
    "longterm = 250 + lookback*2 #idk it's hard to reason about how much I _really_ need, so this should be pretty safe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting up only the stocks that we have seen in-universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if top_imports.use_M:\n",
    "    inUniCount = M[M.time>=train_start_time].groupby('assetCodeId').universe.sum()\n",
    "    stocksInUni = (inUniCount != 0).pipe(lambda x: x.index[x])\n",
    "    assert stocksInUni.is_monotonic_increasing\n",
    "    xStocksInUni = set(stocksInUni) #EDITCELL\n",
    "    inUniCountSeries = inUniCount.astype(float) #EDITCELL\n",
    "    stocksAlways = (inUniCount >= 1981).pipe(lambda x: x.index[x])\n",
    "    xStocksAlways = set(stocksAlways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_basic_features(*,M,delete=True):\n",
    "    M['assetCodeId'] = M.assetCode.map(assetCodeIdAssign)#.astype(int) # much more efficient to process everything as float\n",
    "    if delete:\n",
    "        del M['assetCode']\n",
    "    M['assetNameId'] = M.assetName.map(assetNameIdAssign)#.astype(int)\n",
    "    if delete:\n",
    "        del M['assetName']\n",
    "    for orig_col, new_col in returns_columns.items():\n",
    "        M[new_col] = np.log1p(M[orig_col])\n",
    "        if delete:\n",
    "            del M[orig_col]\n",
    "    # time features\n",
    "    M['dayOfYear'] = M.time.dt.dayofyear.astype(float)\n",
    "    M['dayOfWeek'] = M.time.dt.dayofweek.astype(float)\n",
    "\n",
    "if top_imports.use_M:\n",
    "    set_basic_features(M=M,delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_P(*, F, P, delete=True, target=True, vp=True, always=False):\n",
    "    P['quarter'] = (P.time.dt.year+(P.time.dt.quarter-1)/4).astype(np.float32)\n",
    "    \n",
    "    for c in 'assetCode __0__assetCode __1__assetCode assetName __0__assetName __1__assetName'.split():\n",
    "        if c+'Id' not in P and c in P:\n",
    "            mapper = assetCodeIdAssign if 'Code' in c else assetNameIdAssign\n",
    "            P[c+'Id'] = P[c].map(mapper).astype(np.int32)\n",
    "        if delete and c in P:\n",
    "            del P[c]\n",
    "        \n",
    "    if target:\n",
    "        \n",
    "        if 'y' not in P:\n",
    "            P.rename(columns={'returnsOpenNextMktres10':'y'}, inplace=True)\n",
    "            assert 'y' in P\n",
    "        \n",
    "        P['target'] = P.y>0\n",
    "        P['upDown'] = (P.target*2-1).astype(np.float32)\n",
    "        P['absVal'] = np.abs(P.y)\n",
    "        \n",
    "        # weights\n",
    "        P['no_weight'] = 1.\n",
    "        \n",
    "        if True: #temp defs\n",
    "            P['flat_inUni'] = P.groupby('time').y.transform('count').astype(np.int32)\n",
    "            P['flat_y2'] = P.y**2\n",
    "            P['flat_l2sum'] = np.sqrt(P.groupby('time').flat_y2.transform('sum'))\n",
    "            P['flat_zstd'] = np.sqrt(P.groupby('time').flat_y2.transform('sum') / P.flat_inUni)\n",
    "        P['flat_weight'] = P.absVal\n",
    "        P['flatTotal_weight'] = P.flat_weight / P.flat_l2sum\n",
    "        P['flatNorm_weight'] = P.flat_weight / P.flat_zstd\n",
    "        if delete:\n",
    "            del P['flat_inUni'], P['flat_y2'], P['flat_l2sum'], P['flat_zstd']\n",
    "        \n",
    "        if vp:\n",
    "            for i in [1,10,20,60]:\n",
    "                try:\n",
    "                    if True: #temp defs\n",
    "                        P[f'vp{i}_'] = F[f'vp{i}'] / 1e9\n",
    "                        P[f'vp{i}_y2'] = (P[f'vp{i}_'] * P.y) ** 2\n",
    "                        P[f'vp{i}_l2sum'] = np.sqrt(P.groupby('time')[f'vp{i}_y2'].transform('sum'))\n",
    "                    P[f'vp{i}_weight'] = P.absVal * P[f'vp{i}_']\n",
    "                    P[f'vp{i}Total_weight'] = P[f'vp{i}_weight'] / P[f'vp{i}_l2sum']\n",
    "                    if delete:\n",
    "                        del P[f'vp{i}_'], P[f'vp{i}_y2'], P[f'vp{i}_l2sum']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        \n",
    "        if always:\n",
    "            P['always_weight'] = P.absVal*F.assetCodeId.isin(xStocksAlways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pairs setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from helper.bisect import bisect_left as binary_search\n",
    "\n",
    "# @njit\n",
    "# def index_unique_pairs(A, B, values, repeat):\n",
    "#     assert len(A)==len(B)\n",
    "#     seen = [0] * len(values)\n",
    "#     ans = []\n",
    "#     for i, (a, b) in enumerate(zip(A, B)):\n",
    "#         ia = binary_search(values, a); ib = binary_search(values, b);\n",
    "#         if repeat==-1 or (seen[ia]<repeat and seen[ib]<repeat):\n",
    "#             seen[ia] += 1; seen[ib] += 1;\n",
    "#             ans.append(i)\n",
    "#     return ans\n",
    "\n",
    "# def make_unique_pairs(*, Dl, repeat=-1):\n",
    "#     if repeat==-1:\n",
    "#         return Dl\n",
    "#     ii = index_unique_pairs(Dl[0].values, Dl[1].values, sorted(set(Dl[0].values)|set(Dl[1].values)), repeat=repeat)\n",
    "#     return Dl.iloc[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper.bisect import bisect_left as binary_search\n",
    "\n",
    "@njit\n",
    "def index_unique_pairs(A, B, values, repeat):\n",
    "    assert len(A)==len(B)\n",
    "    seen = [0] * len(values)\n",
    "    ans = []\n",
    "    for i, (a, b) in enumerate(zip(A, B)):\n",
    "        ia = binary_search(values, a); ib = binary_search(values, b);\n",
    "        if repeat==-1 or (seen[ia]<repeat and seen[ib]<repeat):\n",
    "            seen[ia] += 1; seen[ib] += 1;\n",
    "            ans.append(i)\n",
    "    return ans\n",
    "\n",
    "def make_unique_pairs(*, Dl, repeat=-1):\n",
    "    if repeat==-1:\n",
    "        return Dl\n",
    "    ii = index_unique_pairs(Dl[0].values, Dl[1].values, sorted(set(Dl[0].values)|set(Dl[1].values)), repeat=repeat)\n",
    "    return Dl.iloc[ii]\n",
    "\n",
    "def make_pairs(Dl, *, lo, hi=None, repeat=-1):\n",
    "    D = Dl\n",
    "    if hi is None:\n",
    "        hi = (1., 9999)\n",
    "    D = D[(lo[0]<=D.Corr)&(D.Corr<=hi[0])&(lo[1]<=D.Unic)&(D.Unic<=hi[1])]\n",
    "    if isinstance(repeat, int):\n",
    "        D = D.pipe(lambda x: make_unique_pairs(Dl=x, repeat=repeat))\n",
    "    return (D[0].values, D[1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def np_cov(x, y):\n",
    "    '''sigh the Kernel numba doesn't support np.cov yet'''\n",
    "    assert len(x)==len(y)\n",
    "    x_, y_ = x.mean(), y.mean()\n",
    "    return ((x-x_)*(y-y_)).sum()/len(x)\n",
    "\n",
    "@njit\n",
    "def np_corr(x, y):\n",
    "    return np_cov(x, y) / (x.std() * y.std())\n",
    "\n",
    "# @njit\n",
    "# def my_corr(x, y):\n",
    "#     assert len(x)==len(y)\n",
    "#     return (np_cov(x, y) / ( np.sqrt( ((x-x.mean())**2).sum() * ((y-y.mean())**2).sum() ) / len(x) ))\n",
    "\n",
    "@njit\n",
    "def corr_window_at_index(a, cols, labels, index, window, min_periods):\n",
    "    '''Parameters - a: 2D np.ndarray, we are taking corr at row index `index`, going back `window` rows\n",
    "                    cols: tuple of (cols[0],cols[1]) where cols[0/1] is the list of column `labels` of which to take corr of\n",
    "                    labels: the \"names\" of the 0-n column dimensions, that `cols` references\n",
    "                    index: row index which is the last row in the correlation window\n",
    "                    window: int, size of window\n",
    "    Return 2-tuple of np.ndarrays of shape (len(cols[0]),) containing\n",
    "        [0] correlation for every pair of columns in same order as `cols`\n",
    "        [1] common in universe count for the same pair'''\n",
    "    \n",
    "    if index < 0:\n",
    "        index += len(a)\n",
    "    assert index+1 - window >= 0, '{corr_window_at_index} index goes back before window'\n",
    "    \n",
    "    corr = np.full((len(cols[0]),), np.nan, dtype=np.float32)\n",
    "    unic = np.full((len(cols[0]),), np.nan, dtype=np.float32)\n",
    "    \n",
    "    for k in range(len(cols[0])):\n",
    "        j0, j1 = binary_search(labels, cols[0][k]), binary_search(labels, cols[1][k])\n",
    "        x0, x1 = a[index+1-window:index+1, j0], a[index+1-window:index+1, j1]\n",
    "        notnan = ~(np.isnan(x0)|np.isnan(x1))\n",
    "        \n",
    "        sum_notnan = notnan.sum()\n",
    "        if sum_notnan < min_periods:\n",
    "            corr[k] = np.nan\n",
    "            unic[k] = sum_notnan\n",
    "            continue\n",
    "        else:\n",
    "            x0, x1 = x0[notnan], x1[notnan]\n",
    "            guy = np_corr(x0, x1)\n",
    "            corr[k] = guy\n",
    "            unic[k] = sum_notnan\n",
    "    \n",
    "    return corr, unic\n",
    "\n",
    "\n",
    "@njit\n",
    "def corr_windows_at_indices(a, cols, labels, indices, window, min_periods):\n",
    "    '''Parameters - a: 2D np.ndarray, we are taking corr at row index `index`, going back `window` rows\n",
    "                    cols: tuple of (cols[0],cols[1]) where cols[0/1] is the list of column `labels` of which to take corr of\n",
    "                    labels: the \"names\" of the 0-n column dimensions, that `cols` references\n",
    "                    indices: (same shape as `cols`) of row indices, an entry being the last row in the correlation window\n",
    "                    window: int, size of window\n",
    "    Return 2-tuple of np.ndarrays of shape (len(cols[0]),) containing\n",
    "        [0] correlation for every pair of columns in same order as `cols`\n",
    "        [1] common in universe count in the window for the same pair'''\n",
    "        \n",
    "    corr = np.full((len(cols[0]),), np.nan, dtype=np.float32)\n",
    "    unic = np.full((len(cols[0]),), np.nan, dtype=np.float32)\n",
    "    \n",
    "    for k in range(len(cols[0])):\n",
    "        index = indices[k]\n",
    "        \n",
    "        if index < 0:\n",
    "            index += len(a)\n",
    "        assert index+1 - window >= 0, '{corr_window_at_indices} index goes back before window'\n",
    "        \n",
    "        j0, j1 = binary_search(labels, cols[0][k]), binary_search(labels, cols[1][k])\n",
    "        x0, x1 = a[index+1-window:index+1, j0], a[index+1-window:index+1, j1]\n",
    "        notnan = ~(np.isnan(x0)|np.isnan(x1))\n",
    "        \n",
    "        sum_notnan = notnan.sum()\n",
    "        if sum_notnan < min_periods:\n",
    "            corr[k] = np.nan\n",
    "            unic[k] = sum_notnan\n",
    "            continue\n",
    "        else:\n",
    "            x0, x1 = x0[notnan], x1[notnan]\n",
    "            guy = np_corr(x0, x1)\n",
    "            corr[k] = guy\n",
    "            unic[k] = sum_notnan\n",
    "    \n",
    "    return corr, unic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def make_CC_G_Q_from_Dl_F_P_W(*, Dl, F, P, W, lo, hi=None, repeat=-1,\n",
    "#                               roll_corr=list(product(['aoo1','aoo10'],[10,21,62,250]))):\n",
    "#     SEE(f'(pair) len(F.columns) = {len(F.columns)}')\n",
    "    \n",
    "#     D = Dl\n",
    "#     allTime = pd.Series(P.time.unique())\n",
    "#     allTime.index = allTime\n",
    "    \n",
    "#     # |begin| subset the pairs list\n",
    "#     if hi is None:\n",
    "#         hi = (1., 9999)\n",
    "#     D = D[(lo[0]<=D.Corr)&(D.Corr<=hi[0])&(lo[1]<=D.Unic)&(D.Unic<=hi[1])]\n",
    "#     if isinstance(repeat, int):\n",
    "#         D = D.pipe(lambda x: make_unique_pairs(Dl=x, repeat=repeat))\n",
    "# #         w = (D.Corr.fillna(0)*0 + 1.).values # old \"weight\" code, weight before combining the __0__y-__1__y\n",
    "#     elif repeat=='prop':\n",
    "#         assert False, '\"prop\" weighting for pairs not implemented yet!'\n",
    "#         D01 = D\n",
    "#         D10 = D[[1,0,'Corr','Unic']]\n",
    "#         D2 = pd.concat([D01,D10], axis=0)\n",
    "#         aw = D2.groupby(0).Corr.sum()\n",
    "#         w = (D.Corr / np.maximum(D[0].map(aw), D[1].map(aw))).values\n",
    "#         w /= w.mean()\n",
    "#     else:\n",
    "#         assert False\n",
    "#     hgt = sum(p[-1] for p in D.itertuples())\n",
    "#     CC = np.zeros((hgt,2+len(roll_corr)))\n",
    "#     G0, G1 = np.zeros((hgt,len(F.columns))), np.zeros((hgt,len(F.columns)))\n",
    "#     Q0, Q1 = [], []\n",
    "#     # |end| subset the pairs list\n",
    "    \n",
    "#     i = 0\n",
    "#     SEE(f'(pair) generating [{len(D)}]', end='')\n",
    "#     for ii, (_,a0,a1,corr,unic) in enumerate(D.itertuples()):\n",
    "#         print_progress(ii, dot=10, print=SEE)\n",
    "# #         assert len(weight.shape)==0\n",
    "#         in0, in1 = (F.assetCodeId==a0)&(P.universe!=0), (F.assetCodeId==a1)&(P.universe!=0)\n",
    "#         A0, A1 = F[in0], F[in1]\n",
    "#         B0, B1 = P[in0], P[in1]\n",
    "#         inTime = set(B0.time)&set(B1.time)\n",
    "#         tm0, tm1 = B0.time.isin(inTime), B1.time.isin(inTime)\n",
    "#         A0, A1 = A0[tm0], A1[tm1]\n",
    "#         B0, B1 = B0[tm0], B1[tm1]\n",
    "        \n",
    "#         tm = allTime.isin(inTime).values\n",
    "#         def iter_roll_corr():\n",
    "#             for y, r in roll_corr:\n",
    "#                 yield W[y][a0].rolling(window=r, min_periods=5).corr(W[y][a1])[tm] #TODO hard coded min_periods\n",
    "#         unic_ = [np.ones_like(W[next(iter(W))].iloc[:,0][tm])*unic]\n",
    "# #         weight_ = [np.ones_like(W[next(iter(W))].iloc[:,0][tm])*weight]\n",
    "#         C = np.stack(chain(iter_roll_corr(),unic_), axis=1)\n",
    "        \n",
    "#         G0[i:i+unic], G1[i:i+unic] = A0.values, A1.values\n",
    "#         Q0.append(B0); Q1.append(B1);\n",
    "#         CC[i:i+unic] = C\n",
    "        \n",
    "#         i += unic\n",
    "#     SEE()\n",
    "    \n",
    "#     Fcols = F.columns\n",
    "#     del Dl, F, P, W; gc.collect()\n",
    "        \n",
    "#     Q0, Q1 = pd.concat(Q0, axis=0), pd.concat(Q1, axis=0)\n",
    "#     G0, G1 = pd.DataFrame(G0, index=Q0.index, columns=Fcols), pd.DataFrame(G1, index=Q1.index, columns=Fcols)\n",
    "#     CC = pd.DataFrame(CC, columns=[f'__corr__{y}_{r}' for y,r in roll_corr]+['bothInUniCount'])\n",
    "#     for x in [CC, G0, G1, Q0, Q1]:\n",
    "#         convert32(x)\n",
    "#     return CC, (G0,G1), (Q0,Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_CC_G_Q(pairs, *, F, P, W, labels=None, roll_corr=list(product(['aoo','aooTEN','aooTTY'],[10,21,62,125,250]))):\n",
    "#     print('___ make_CC_G_Q')\n",
    "#     print('>>> pairs[0]', pairs[0].dtype)\n",
    "    pairId = pairs[0]*100_000+pairs[1]\n",
    "#     print('>>> pairId', pairId.dtype)\n",
    "    single = P.time.nunique()==1\n",
    "    if single:\n",
    "        I0 = pd.DataFrame({'assetCodeId': pairs[0], 'pairId': pairId}, dtype=np.int32)\n",
    "#         print('>>> I0', I0.dtypes)\n",
    "        I1 = pd.DataFrame({'assetCodeId': pairs[1], 'pairId': pairId}, dtype=np.int32)\n",
    "#         print('>>> I1', I1.dtypes)\n",
    "        # merge on P to get the intersected guys\n",
    "        Q0, Q1 = I0.merge(P, on='assetCodeId', how='inner'), I1.merge(P, on='assetCodeId', how='inner')\n",
    "#         print('>>> Q0', Q0.dtypes)\n",
    "#         print('>>> Q1', Q1.dtypes)\n",
    "        # merge on intersected\n",
    "        Q0 = Q1[['pairId']].merge(Q0, how='inner'); Q1 = Q0[['pairId']].merge(Q1, how='left');\n",
    "#         print('>>> Q0', Q0.dtypes)\n",
    "#         print('>>> Q1', Q1.dtypes)\n",
    "        # make intersected set for merging F\n",
    "        G0 = Q0[['assetCodeId']].merge(F, on='assetCodeId', how='left')\n",
    "#         print('>>> G0', G0.dtypes)\n",
    "        G1 = Q1[['assetCodeId']].merge(F, on='assetCodeId', how='left')\n",
    "#         print('>>> G1', G1.dtypes)\n",
    "        # corr_window_at_index(a, cols, labels, index, window, min_periods=10)\n",
    "        \n",
    "        pairs = G0.assetCodeId.values.astype(np.int32), G1.assetCodeId.values.astype(np.int32)\n",
    "#         print('>>> !!!', G0.assetCodeId.values.dtype, G1.assetCodeId.values.dtype)\n",
    "        def iter_corr():\n",
    "            for c, h in roll_corr:\n",
    "                if isinstance(W[c], pd.DataFrame):\n",
    "                    index = binary_search(W[c].index.values, P.time.values[-1])\n",
    "                    assert W[c].columns.is_monotonic_increasing, 'W[c].columns not monotonic increasing'\n",
    "                    yield corr_window_at_index(W[c].values, pairs, W[c].columns.values, index, h, min_periods=10)\n",
    "                elif isinstance(W[c], np.ndarray):\n",
    "                    index = -1\n",
    "                    yield corr_window_at_index(W[c], pairs, labels, index, h, min_periods=10)\n",
    "                else:\n",
    "                    assert False, 'W[c] datatype wrong'\n",
    "        #BUG.guy = guy = list(iter_corr())\n",
    "        CC = np.stack(list(flatten(iter_corr())), axis=1)\n",
    "#         print('>>> CC', CC.dtype)\n",
    "        #TODO inside CC, bothInUniCount i.e. unic and weight #DONE actually forget weight we will only use flat weight\n",
    "    else:\n",
    "        # make time list\n",
    "        P['_mt'] = P.time.astype(int)\n",
    "#         print('>>> P', P.dtypes)\n",
    "        times = P.time.unique()\n",
    "        x_times = {t: i for i, t in enumerate(times)}\n",
    "        _mts = P._mt.unique()\n",
    "        \n",
    "        # make canonical index template\n",
    "        def _index(pair):\n",
    "            a = np.asarray(list(product(_mts, pair)))\n",
    "            b = np.asarray(list(product(_mts, pairId)))\n",
    "            return {'_mt': a[:,0], 'assetCodeId': a[:,1].astype(np.int32), 'pairId': b[:,1].astype(np.int32)}\n",
    "        I0, I1 = pd.DataFrame(_index(pairs[0])), pd.DataFrame(_index(pairs[1]))\n",
    "#         print('>>> I0', I0.dtypes)\n",
    "#         print('>>> I1', I1.dtypes)\n",
    "        # do P first to get merge columns\n",
    "        Q0, Q1 = I0.merge(P,on=['_mt','assetCodeId'],how='inner'), I1.merge(P,on=['_mt','assetCodeId'],how='inner')\n",
    "#         print('>>> Q0', Q0.dtypes)\n",
    "#         print('>>> Q1', Q1.dtypes)\n",
    "        # get common rows\n",
    "        Q0 = Q1[['_mt','pairId']].merge(Q0, how='inner'); Q1 = Q0[['_mt','pairId']].merge(Q1, how='left');\n",
    "#         print('>>> Q0', Q0.dtypes)\n",
    "#         print('>>> Q1', Q1.dtypes)\n",
    "        # now do F\n",
    "        F['_mt'] = P._mt\n",
    "#         print('>>> F', F.dtypes)\n",
    "        G0 = Q0[['_mt','assetCodeId']].merge(F, on=['_mt','assetCodeId'], how='left')\n",
    "#         print('>>> G0', G0.dtypes)\n",
    "        G1 = Q1[['_mt','assetCodeId']].merge(F, on=['_mt','assetCodeId'], how='left')\n",
    "#         print('>>> G1', G1.dtypes)\n",
    "        del F['_mt'], P['_mt'], Q0['_mt'], Q1['_mt'], G0['_mt'], G1['_mt']\n",
    "        \n",
    "        pairs = G0.assetCodeId.values.astype(np.int32), G1.assetCodeId.values.astype(np.int32)\n",
    "#         print('>>> !!!', G0.assetCodeId.values.dtype, G1.assetCodeId.values.dtype)\n",
    "        def iter_corr():\n",
    "            for c, h in roll_corr:\n",
    "                if isinstance(W[c], pd.DataFrame):\n",
    "                    mapper = dict(zip(W[c].index, np.arange(len(W[c].index))))\n",
    "                    indices = Q0.time.map(mapper).values\n",
    "                    BUG.W, BUG.c = W, c\n",
    "                    assert W[c].columns.is_monotonic_increasing, 'W[c].columns not monotonic increasing'\n",
    "                    yield corr_windows_at_indices(W[c].values, pairs, W[c].columns.values, indices, h, min_periods=10)\n",
    "                elif isinstance(W[c], np.ndarray):\n",
    "                    indices = (Q0.time.map(x_times) - len(x_times)).values\n",
    "                    yield corr_windows_at_indices(W[c], pairs, labels, indices, h, min_periods=10)\n",
    "                else:\n",
    "                    assert False, 'W[c] data type wrong'\n",
    "        BUG.guy = guy = list(flatten(iter_corr()))\n",
    "        CC = np.stack(guy, axis=1)\n",
    "#         print('>>> CC', CC.dtype)\n",
    "    assert len(CC)==len(G0)==len(Q0), 'lengths of CC G[0/1] Q[0/1] need to be the same'\n",
    "    \n",
    "    G0.reset_index(inplace=True, drop=True); G1.reset_index(inplace=True, drop=True);\n",
    "    Q0.reset_index(inplace=True); Q1.reset_index(inplace=True);\n",
    "    return (\n",
    "        pd.DataFrame(CC,columns=flatten([f'__corr__{y}_{r}',f'__unic__{y}_{r}'] for y,r in roll_corr)),\n",
    "        (G0, G1),\n",
    "        (Q0, Q1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_FG_PQ(copy_filter, diff_filter, plus_filter, corr_filter, *, CC, G, Q):\n",
    "#     print('___ make_FG_PQ')\n",
    "    SEE('(pair) putting together...', end=' ')\n",
    "#     print('>>> CC', CC.dtypes)\n",
    "#     print('>>> G[0]', G[0].dtypes)\n",
    "#     print('>>> G[1]', G[1].dtypes)\n",
    "#     print('>>> Q[0]', Q[0].dtypes)\n",
    "#     print('>>> Q[1]', Q[1].dtypes)\n",
    "    # weight = CC['weight'].values # old code\n",
    "    \n",
    "    Gcols, Qcols = G[0].columns, Q[0].columns\n",
    "    assert (Gcols==G[1].columns).all() and (Qcols==Q[1].columns).all()\n",
    "    \n",
    "    \n",
    "    cop, dif, pls = list(filter(copy_filter, Gcols)), list(filter(diff_filter, Gcols)), list(filter(plus_filter, Gcols))\n",
    "    G0, G1 = G[0][cop].values, G[1][cop].values\n",
    "#     print('>>> G0', G0.dtype)\n",
    "#     print('>>> G1', G1.dtype)\n",
    "    H01 = G[0][dif].values.astype(np.float32) - G[1][dif].values.astype(np.float32)\n",
    "#     print('>>> H01', H01.dtype)\n",
    "    I01 = G[0][pls].values.astype(np.float32) + G[1][pls].values.astype(np.float32)\n",
    "#     print('>>> I01', I01.dtype)\n",
    "    CC = CC[list(filter(corr_filter, CC.columns))]\n",
    "#     print('>>> CC', CC.dtypes)\n",
    "#     print('>>> CC.values', CC.values.dtype)\n",
    "    \n",
    "    F01 = np.concatenate([G0,G1,H01,I01,CC.values.astype(np.float32)], axis=1)\n",
    "    F10 = np.concatenate([G1,G0,-H01,I01,CC.values.astype(np.float32)], axis=1)\n",
    "#     print('>>> F01', F01.dtype)\n",
    "#     print('>>> F10', F10.dtype)\n",
    "    FG = np.concatenate([F01, F10], axis=0)\n",
    "#     print('>>> FG', FG.dtype)\n",
    "    if not np.issubdtype(FG.dtype, np.float):\n",
    "        FG = FG.astype(np.float32)\n",
    "#     print('>>> FG', FG.dtype)\n",
    "    \n",
    "    FGcols = (['__0__'+c for c in cop] + ['__1__'+c for c in cop] + ['__0-1__'+c for c in dif]\n",
    "              + ['__0+1__'+c for c in pls] + list(CC.columns))\n",
    "    FG = pd.DataFrame(FG, columns=FGcols, copy=False)\n",
    "#     print('>>> FG', FG.dtypes)\n",
    "    \n",
    "    \n",
    "    Q = Q[0].reset_index(drop=True), Q[1].reset_index(drop=True)\n",
    "#     print('>>> Q', Q[0].dtypes, Q[1].dtypes)\n",
    "    assert(Q[0].time==Q[1].time).all()\n",
    "    \n",
    "    #TODO again we don't need universe here\n",
    "    Q01 = pd.DataFrame(dict(time=Q[0].time,\n",
    "        **{f'__0__{c}': Q[0][c] for c in ['assetCode','assetCodeId','assetName','assetNameId'] if c in Q[0]},\n",
    "        **{f'__1__{c}': Q[1][c] for c in ['assetCode','assetCodeId','assetName','assetNameId'] if c in Q[1]}))\n",
    "    if 'y' in Q[0]:\n",
    "        assert 'y' in Q[1]\n",
    "        Q01['y'] = Q[0].y - Q[1].y\n",
    "    Q10 = pd.DataFrame(dict(time=Q[1].time,\n",
    "        **{f'__0__{c}': Q[1][c] for c in ['assetCode','assetCodeId','assetName','assetNameId'] if c in Q[1]},\n",
    "        **{f'__1__{c}': Q[0][c] for c in ['assetCode','assetCodeId','assetName','assetNameId'] if c in Q[0]}))\n",
    "    if 'y' in Q[1]:\n",
    "        assert 'y' in Q[0]\n",
    "        Q10['y'] = Q[1].y - Q[0].y\n",
    "#     print('>>> Q01', Q01.dtypes)\n",
    "#     print('>>> Q10', Q10.dtypes)\n",
    "    \n",
    "    PQ = pd.concat([Q01, Q10], axis=0, ignore_index=True)\n",
    "#     PQ['pair_weight'] = 1 # np.concatenate([weight, weight], axis=0) # old code\n",
    "#     print('>>> PQ', PQ.dtypes)\n",
    "    \n",
    "    PQ.sort_values('time', inplace=True)\n",
    "    FG = FG.reindex(index=PQ.index, copy=False)\n",
    "    \n",
    "    FG.reset_index(drop=True, inplace=True)\n",
    "    PQ.reset_index(drop=True, inplace=True)\n",
    "    setup_P(F=None, P=PQ, target=True, vp=False, always=False)\n",
    "    \n",
    "    SEE('(pair) done')\n",
    "    return FG, PQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
