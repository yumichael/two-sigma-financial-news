{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "978bf991612af424bebf2c7a03a8cdb7ed10156f"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c670dede52f15ec87e9f5246d9ca8dfb61313d1",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "market_train_df.shape, news_train_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa3c702fabdf93b2c1b578ffddeba8939aba8c8a"
      },
      "cell_type": "code",
      "source": "market_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "677aa3a82b8f44b31fa8f6511df03b04faddd767"
      },
      "cell_type": "code",
      "source": "market_train_df = market_train_df\nnews_train_df = news_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e8a39842068a8a615e61ac548091cf3aa6b609f"
      },
      "cell_type": "code",
      "source": "# import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom sklearn.linear_model import SGDRegressor\n#from fancyimpute import simple_fill, IterativeImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.cluster.bicluster import SpectralCoclustering\n\nfrom xgboost.sklearn import XGBRegressor, XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# from sklearn.linear_model import HuberRegressor\nfrom sklearn.ensemble import VotingClassifier\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a0869f29b6636e0b5d4ea68e1074b5f6f1b6920e"
      },
      "cell_type": "code",
      "source": "from itertools import chain\n\ndef fast_flatten(input_list):\n    return list(chain.from_iterable(input_list))\n\ndef fast_concat(frames):\n    COLUMN_NAMES = frames[0].columns\n    df_dict = dict.fromkeys(COLUMN_NAMES, [])\n    for col in COLUMN_NAMES:\n        # Use a generator to save memory\n        extracted = (frame[col] for frame in frames)\n\n        # Flatten and save to df_dict\n        df_dict[col] = fast_flatten(extracted)\n    return(pd.DataFrame.from_dict(df_dict)[COLUMN_NAMES])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b8ab1127fb3c5e87a3bc056d25b9ad762af2ec8"
      },
      "cell_type": "code",
      "source": "news_cols_agg = {\n#     'urgency': ['min', 'count'],\n    #'takeSequence': ['max'],\n    #'bodySize': ['min', 'max', 'mean', 'std'],\n    #'wordCount': ['min', 'max', 'mean', 'std'],\n    #'sentenceCount': ['min', 'max', 'mean', 'std'],\n    #'companyCount': ['min', 'max', 'mean', 'std'],\n#     'marketCommentary': ['min', 'max', 'mean', 'std'],\n#     'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n#     'sentimentNegative': ['median'],\n    #'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n#     'sentimentPositive': ['median'],\n    #'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n#     'noveltyCount12H': ['median']\n    #'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    #'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    #'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    #'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n#     'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    #'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    #'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    #'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43998a71dd98fe561c4a83ee4e581cd71b4a2cb3"
      },
      "cell_type": "code",
      "source": "def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n#     market_train_df['diff_open_close'] = (market_train_df['close'] - market_train_df['open'])/market_train_df['open']\n    market_train_df\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df\n\ndef join_market_news(market_train_df, news_train_df):    \n    return market_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b186c86e54a917a2a44d1ccc73dab587ef58d1f9"
      },
      "cell_type": "code",
      "source": "df = market_train_df[['assetCode', 'universe']].groupby('assetCode').max()\nto_drop = df[df.universe==0].index\nprint(to_drop)\n#market_train_df = market_train_df[~market_train_df.assetCode.isin(to_drop)]\n# news_train_df = news_train_df[~news_train_df.assetCodes.isin(to_drop)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa85b9286efaa40c825b5236e5a18e940c5bf68b"
      },
      "cell_type": "code",
      "source": "def get_xy(market_train_df, news_train_df, le=None):\n    y = market_train_df[market_train_df.time.dt.year>2008]['returnsOpenNextMktres10']#.clip(-1, 1)\n    #y = market_train_df['returnsOpenNextMktres10']\n    x, le = get_x(market_train_df, news_train_df)\n    return x, y, le\n\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n#     le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n\n\ndef get_x2(market_train_df, news_train_df, le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    # If not label-encoder... encode assetCode\n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    print(x.columns)\n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n#     try:\n#         x.drop(columns=['universe'], inplace=True)\n#     except:\n#         pass\n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    \n    x = x[x.time.dt.year > 2008]\n    \n    #x.drop(columns='time', inplace=True)\n    #x.set_index('time', inplace=True)\n    x.drop(columns='assetName', inplace=True)\n#    x.fillna(-1000,inplace=True)\n\n    # Fix some mixed-type columns\n#     for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n#         x[bogus_col] = x[bogus_col].astype(float)\n    # x = x.fillna(0)\n    return x, (le_assetCode, le_assetName)\n\ndef get_x(market_train_df, news_train_df, le=None):\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n    x = market_train_df.copy()\n    \n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        le_assetCode, le_assetName = le\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n#     x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    \n    x = x[x.time.dt.year > 2008].drop(columns='assetName', inplace=False)\n#     x.drop(columns='assetName', inplace=True)\n    return x, (le_assetCode, le_assetName)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04513167c22cc0af9037e355e88c6ab7b2316eb0",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "%%time\n\n# This will take some time...\nX, y, le = get_xy(market_train_df, news_train_df)\n\ndynamic_X = X.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4398034e1d17df7c98c3240c9cf879f38eed6da"
      },
      "cell_type": "code",
      "source": "X.shape, y.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b1f4246fbb02356d1c7219b2fc59ee1a94b473e"
      },
      "cell_type": "code",
      "source": "X.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "508eeb55544ae38df36d18f3b94a295c5ae25810"
      },
      "cell_type": "code",
      "source": "n_train = int(X.shape[0] * 0.8)\nuniverse = market_train_df[market_train_df.time.dt.year>2008]['universe']\n#universe = market_train_df['universe']\n\n\ntime = market_train_df[market_train_df.time.dt.year>2008]['time']\n#time = market_train_df['time']\nt_valid = time.iloc[n_train:]\n\n\n# Free memory\n#del market_train_df, news_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51809729f8c84d2c6764ff701677ff98cb894eb9"
      },
      "cell_type": "code",
      "source": "len(t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2cab92de9aa7aaf228dc4fdd36a11ae400caf1e1"
      },
      "cell_type": "code",
      "source": "u_valid = universe[universe > 0].index\ngood_universe_index = [x in u_valid for x in t_valid.index]\n#t_valid_ = time.iloc[[x for x in t_valid.index if x in u_valid]]\nt_valid_ = time.iloc[good_universe_index]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23660a050720555d1ba92927877017826dc21f60"
      },
      "cell_type": "code",
      "source": "len(t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9432a6508d27ec33ee1dab88d542ec27064d25fa"
      },
      "cell_type": "code",
      "source": "X_ = X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7988288a10241f36b0ff677b62b970c8a37d5dc9"
      },
      "cell_type": "code",
      "source": "X = X_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "950fa264beca461278f638855b7b2ecd02ce1bdd"
      },
      "cell_type": "code",
      "source": "X.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b1d8b9d62ea87d94e1da4c3dad3f3a8a039474b",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "X.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "88a681d8757f5b83c47560c438945b8939f2aa41"
      },
      "cell_type": "code",
      "source": "X.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a625fd81cc0b2f4733e7296efda1f48e6497aef"
      },
      "cell_type": "code",
      "source": "assets_to_drop = X.groupby('assetCode')[['returnsOpenPrevMktres10']].last()\nassets_to_drop = list(assets_to_drop[np.isnan(assets_to_drop.returnsOpenPrevMktres10)].index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9ed897c1b3bdd519b09d07d538401881835ad3c"
      },
      "cell_type": "code",
      "source": "cluster_X = X[['assetCode', 'time', 'returnsOpenPrevMktres10']].copy()\ncluster_X = cluster_X[~cluster_X.assetCode.isin(assets_to_drop)]\ncluster_X = cluster_X[cluster_X.assetCode != -1]\ncluster_X = cluster_X[cluster_X.time.dt.year > 2015]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e40dd1c954717df0b21d68d34d12fa472336064",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "correl = cluster_X.pivot(index='time', columns='assetCode', values='returnsOpenPrevMktres10').dropna(axis=1, thresh=200).corr()\ncovar = cluster_X.pivot(index='time', columns='assetCode', values='returnsOpenPrevMktres10').dropna(axis=1, thresh=200).cov()\n\n\nmodel = SpectralCoclustering(n_clusters=5, random_state=0, svd_method='arpack')\nmodel.fit(correl)\n\ncorrel_cluster = dict(zip(correl.columns, model.row_labels_))\n#print(np.argsort(model.row_labels_))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e3fe50f7c42128601e91add309aa97394771ae7b"
      },
      "cell_type": "code",
      "source": "cluster_X = X[['assetCode', 'time', 'returnsOpenPrevMktres10']].copy()\ncluster_X = cluster_X[~cluster_X.assetCode.isin(assets_to_drop)]\ncluster_X = cluster_X[cluster_X.time.dt.year > 2014].dropna()\nweights = cluster_X.groupby('assetCode').std()\nweights = weights.fillna(weights.median()).returnsOpenPrevMktres10\nweights = weights.to_dict()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9590b8d06c83e51a28b8e3a4203776e00e32a67b"
      },
      "cell_type": "code",
      "source": "#pd.get_dummies(X['assetCode'].map(correl_cluster).fillna(100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27e6697709f175a63472929f1e52d7ddf045a377"
      },
      "cell_type": "code",
      "source": "# import seaborn as sns\n# sns.clustermap(to_cluster)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "720ab007027b194c11b4eac1224578f5aeaed702"
      },
      "cell_type": "code",
      "source": "# from sklearn.cluster.bicluster import SpectralCoclustering\n# import matplotlib.pyplot as plt\n\n# model = SpectralCoclustering(n_clusters=18, random_state=0, svd_method='arpack')\n# model.fit(to_cluster)\n\n# correl_cluster = dict(zip(to_cluster.columns, model.row_labels_))\n# print(np.argsort(model.row_labels_))\n# fit_data = to_cluster[np.argsort(model.row_labels_)].T\n# fit_data = fit_data[np.argsort(model.column_labels_)].T\n# #fit_data = fit_data[:, (np.argsort(model.column_labels_) + 1)]\n\n# plt.matshow(fit_data, cmap=plt.cm.Blues)\n# plt.title(\"Spectral Co-Clustering of Average Correlation Matrix\")\n\n# #plt.show()\n# #plt.savefig()\n\n# fig = plt.gcf()\n# fig.set_size_inches(18.5, 10.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d330882cf409ab7c351f96b9b3b7301d0e4dd69"
      },
      "cell_type": "code",
      "source": "import numba\n\n@numba.jit\ndef RSI(df, n):\n    delta = df.returnsOpenPrevMktres1\n    #-----------\n    dUp, dDown = delta.copy().fillna(0), delta.copy().fillna(0)\n    dUp[dUp <= 0] = 0\n    dDown[dDown > 0] = 0\n    \n    RolUp = dUp.rolling(n).mean()\n    RolDown = dDown.rolling(n).mean().abs()\n\n    RS = RolUp / RolDown\n    rsi= pd.Series(100.0 - (100.0 / (1.0 + RS)), name='RSI_')\n    df = df.join(rsi)\n    return df\n \n@numba.jit\ndef kst_oscillator(df, r1=5, r2=10, r3=15, r4=20, n1=10, n2=10, n3=10, n4=15):\n    \"\"\"Calculate KST Oscillator for given data.\n    :return: pandas.DataFrame\n    \"\"\"\n    serie = df['returnsOpenPrevMktres1'].cumsum()\n    M = serie.diff(r1 - 1)\n    N = serie.shift(r1 - 1)\n    ROC1 = M / N\n    M = serie.diff(r2 - 1)\n    N = serie.shift(r2 - 1)\n    ROC2 = M / N\n    M = serie.diff(r3 - 1)\n    N = serie.shift(r3 - 1)\n    ROC3 = M / N\n    M = serie.diff(r4 - 1)\n    N = serie.shift(r4 - 1)\n    ROC4 = M / N\n    KST = pd.Series(\n        ROC1.rolling(n1).sum() + ROC2.rolling(n2).sum() * 2 + ROC3.rolling(n3).sum() * 3 + ROC4.rolling(n4).sum() * 4,\n        name='KST')\n    df = df.join(KST)\n    return df\n\n@numba.jit\ndef true_strength_index(df, r=3, s=6):\n    \"\"\"Calculate True Strength Index (TSI) for given data.\n    \"\"\"\n    #M = pd.Series(df['close'].diff(1))\n    M = pd.Series(df['returnsOpenPrevMktres1'])\n    aM = abs(M)\n    EMA1 = pd.Series(M.ewm(span=r, min_periods=r).mean())\n    aEMA1 = pd.Series(aM.ewm(span=r, min_periods=r).mean())\n    EMA2 = pd.Series(EMA1.ewm(span=s, min_periods=s).mean())\n    aEMA2 = pd.Series(aEMA1.ewm(span=s, min_periods=s).mean())\n    TSI = pd.Series(EMA2 / aEMA2, name='TSI_')\n    df = df.join(TSI)\n    return df\n\n@numba.jit\ndef on_balance_volume(df, n=6):\n    \"\"\"Calculate On-Balance Volume for given data.\n    \"\"\"\n    OBV = df.volume.copy()\n    OBV = OBV*np.sign(df.returnsOpenPrevMktres1)\n    OBV = pd.Series(OBV)\n    OBV_ma = pd.Series(OBV.rolling(n, min_periods=n).mean(), name='OBV_')\n    df = df.join(OBV_ma)\n    return df\n\n@numba.jit\ndef force_index(df, n=6):\n    \"\"\"Calculate Force Index for given data.\n    \"\"\"\n    F = pd.Series(df['close'].diff(n) * df['volume'].diff(n), name='Force_' + str(n))\n    df = df.join(F)\n    return df\n\n@numba.jit\ndef coppock_curve(df, n=8):\n    \"\"\"Calculate Coppock Curve for given data.\n    \"\"\"\n    serie = df['returnsOpenPrevMktres1'].cumsum()\n    M = serie.diff(int(n * 11 / 10) - 1)\n    N = serie.shift(int(n * 11 / 10) - 1)\n    ROC1 = M / N\n    M = serie.diff(int(n * 14 / 10) - 1)\n    N = serie.shift(int(n * 14 / 10) - 1)\n    ROC2 = M / N\n    Copp = pd.Series((ROC1 + ROC2).ewm(span=n, min_periods=n).mean(), name='Copp')\n    df = df.join(Copp)\n    return df\n\n@numba.jit\ndef shifts(df):\n    df = df.join(pd.Series(df['returnsOpenPrevMktres1'].shift(1), name='shift_1'))\n    df = df.join(pd.Series(df['returnsOpenPrevMktres1'].shift(2), name='shift_2'))\n    df = df.join(pd.Series(df['returnsOpenPrevMktres1'].shift(3), name='shift_3'))\n    df = df.join(pd.Series(df['returnsOpenPrevMktres1'].shift(4), name='shift_4'))\n    return(df)\n\n@numba.jit\ndef shift_10(df):\n    df = df.join(pd.Series(df['returnsOpenPrevMktres10'].shift(10), name='shift_10_1'))\n    return(df)\n\n@numba.jit\ndef shift_10_2(df):\n    return(df['returnsOpenPrevMktres10'].shift(10))\n\n@numba.jit\ndef shift_20(df):\n    df = df.join(pd.Series(df['returnsOpenPrevMktres10'].shift(20), name='shift_10_2'))\n    return(df)\n\n@numba.jit\ndef shift_30(df):\n    df = df.join(pd.Series(df['returnsOpenPrevMktres10'].shift(30), name='shift_10_3'))\n    return(df)\n\n@numba.jit\ndef macd(df, n_fast, n_slow):\n    \"\"\"Calculate MACD, MACD Signal and MACD difference\n    \"\"\"\n    EMAfast = pd.Series(df['returnsOpenPrevMktres1'].ewm(span=n_fast, min_periods=n_slow).mean())\n    EMAslow = pd.Series(df['returnsOpenPrevMktres1'].ewm(span=n_slow, min_periods=n_slow).mean())\n    MACD = pd.Series(EMAfast - EMAslow, name='MACD_')\n    MACDsign = pd.Series(MACD.ewm(span=9, min_periods=9).mean(), name='MACDsign_')\n    MACDdiff = pd.Series(MACD - MACDsign, name='MACDdiff_')\n    df = df.join(MACD)\n    df = df.join(MACDsign)\n    df = df.join(MACDdiff)\n    return df\n\n@numba.jit\ndef macd_avg(df, n_fast, n_slow):\n    \"\"\"Calculate MACD, MACD Signal and MACD difference\n    \"\"\"\n    EMAfast = pd.Series(df['returnsOpenPrevMktres1'].rolling(n_fast).mean())\n    EMAslow = pd.Series(df['returnsOpenPrevMktres1'].rolling(n_slow).mean())\n    MACD = pd.Series(EMAfast - EMAslow, name='MACD_')\n    MACDsign = pd.Series(MACD.ewm(span=9, min_periods=9).mean(), name='MACDsign_')\n    MACDdiff = pd.Series(MACD - MACDsign, name='MACDdiff_')\n#     df = df.join(MACD)\n#     df = df.join(MACDsign)\n    df = df.join(MACDdiff)\n    return df\n\n@numba.jit\ndef macd_avg2(df, n_fast, n_slow):\n    \"\"\"Calculate MACD, MACD Signal and MACD difference\n    \"\"\"\n    EMAfast = df['returnsOpenPrevMktres1'].rolling(n_fast).mean()\n    EMAslow = df['returnsOpenPrevMktres1'].rolling(n_slow).mean()\n    MACD = EMAfast - EMAslow\n    MACDsign = MACD.ewm(span=9, min_periods=9).mean()\n    MACDdiff = MACD - MACDsign\n#     df = df.join(MACD)\n#     df = df.join(MACDsign)\n    #df = df.join(MACDdiff)\n    return MACDdiff\n\n@numba.jit\ndef ewma(df, n):\n    \"\"\"Calculate MACD, MACD Signal and MACD difference\n    \"\"\"\n    EMA = pd.Series(df['returnsOpenPrevMktres1'].ewm(span=n, min_periods=n).mean(), name='ewma')\n    df = df.join(EMA)\n    return df\n\n@numba.jit\ndef trix(df, n):\n    \"\"\"Calculate TRIX for given data.\n    \"\"\"\n    price = (1+df['returnsOpenPrevMktres1']).cumprod()\n    EX1 = price.ewm(span=n, min_periods=n).mean()\n    EX2 = EX1.ewm(span=n, min_periods=n).mean()\n    EX3 = EX2.ewm(span=n, min_periods=n).mean()\n    i = 0\n    ROC_l = [np.nan]\n    l = len(df.index)\n    ROC_l = (EX3 - EX3.shift(1))/EX3.shift(1)\n#     while i + 1 < l:#df.index[-1]:\n#         ROC = (EX3.iloc[i+1] - EX3.iloc[i]) / EX3.iloc[i]\n#         ROC_l.append(ROC)\n#         i = i + 1\n    Trix = pd.Series(ROC_l, name='Trix_')\n    df = df.join(Trix)\n    return df\n\n\n@numba.jit\ndef std_rol(df, n):\n    df = df.join(pd.Series(df['returnsOpenPrevMktres1'].rolling(n).std(), name='std'))\n    return(df)\n\n@numba.jit\ndef stochastic_oscillator_d(df, n):\n    \"\"\"Calculate stochastic oscillator %D for given data.\n    :param df: pandas.DataFrame\n    :param n: \n    :return: pandas.DataFrame\n    \"\"\"\n    serie = df['returnsOpenPrevMktres1'].cumsum()\n    SOk = pd.Series((serie - serie.rolling(10).min()) / (serie.rolling(10).max() - serie.rolling(10).min()), name='SO%k')\n    SOd = pd.Series(SOk.ewm(span=n, min_periods=n).mean(), name='SOd')\n    df = df.join(SOd)\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b53d51e30311e951137a5ef16f7acd7e99bbe1c",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# X[['assetCode', 'returnsClosePrevRaw1']].groupby('assetCode')[['returnsClosePrevRaw1']].apply(RSI)\n# volume = X[['assetCode', 'volume', 'close', 'returnsOpenPrevMktres1', 'returnsOpenPrevMktres10']].groupby('assetCode')\n# volume.apply(std_rol).dropna()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19d670118a77e4847dfcd1b983d6ef73ea592876"
      },
      "cell_type": "code",
      "source": "%%time \n\ndef add_features(X, clusters=None):\n    \n#     volume = X[['assetCode', 'volume', 'close']].copy()\n#     volume.volume += 10000000\n#     volume = volume.groupby('assetCode')\n#     X['volume_diff'] = volume[['volume']].pct_change().sort_index()\n#     X['volume_MACD_6_3'] = volume[['volume']].pct_change().rolling(3).mean() - volume[['volume']].pct_change().rolling(6).mean()\n#     X['copp'] = volume.apply(coppock_curve)[['Copp_6']]\n#     X['force'] = volume.apply(force_index)[['Force_6']]\n#     X['OBV'] = volume.apply(on_balance_volume)[['OBV_6']]\n#     X['TSI'] = volume.apply(true_strength_index)[['TSI_3_6']]\n#     X['KST'] = volume.apply(kst_oscillator)[['KST']]\n#     del volume\n     \n    \n    #roll_asset = X[['assetCode', 'returnsClosePrevRaw1']].groupby('assetCode')[['returnsClosePrevRaw1']]\n    #X['return_close_M6'] = roll_asset.rolling(6).mean().reset_index(0,drop=True).sort_index()\n    #X['return_close_M3'] = roll_asset.rolling(3).mean().reset_index(0,drop=True).sort_index()\n    #X['return_MACD_6_3'] = X['return_close_M3'] - X['return_close_M6']\n    #del roll_asset\n#     X = X.loc[:, ['assetCode', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsOpenPrevRaw10', 'volume']]\n    \n#     roll_mkt = X[['assetCode', 'returnsOpenPrevMktres1', 'returnsOpenPrevMktres10', 'volume']].groupby('assetCode')\n#     X['mkt_close_M6'] = roll_mkt[['returnsOpenPrevMktres1']].rolling(6).mean().reset_index(0,drop=True).sort_index()\n#     X['mkt_close_M3'] = roll_mkt[['returnsOpenPrevMktres1']].rolling(3).mean().reset_index(0,drop=True).sort_index()\n#     X['mkt_MACD_6_3'] = X['mkt_close_M3'] - X['mkt_close_M6']\n#     X[['MACD_3_6', 'MACD_3_6_sign', 'MACD_3_6_diff']] = roll_mkt.apply(macd_avg, n_fast=3, n_slow=6)[['MACD_', 'MACDsign_', 'MACDdiff_']]\n#     X['mkt_close_M6'] = roll_mkt[['returnsClosePrevMktres1']].rolling(12).mean().reset_index(0,drop=True).sort_index()\n#     X['mkt_close_M3'] = roll_mkt[['returnsClosePrevMktres1']].rolling(6).mean().reset_index(0,drop=True).sort_index()\n#     X['mkt_MACD_12_6'] = X['mkt_close_M3'] - X['mkt_close_M6']\n#     X.loc[:, 'mkt_MACD_6_3'] = roll_mkt[['returnsClosePrevMktres1']].rolling(3).mean().reset_index(0,drop=True).sort_index() - roll_mkt[['returnsClosePrevMktres1']].rolling(6).mean().reset_index(0,drop=True).sort_index() \n#     X.loc[:, 'mkt_close_M24'] = roll_mkt[['returnsClosePrevMktres1']].rolling(24).mean().reset_index(0,drop=True).sort_index()\n#     X.loc[:, 'mkt_close_M12'] = roll_mkt[['returnsClosePrevMktres1']].rolling(12).mean().reset_index(0,drop=True).sort_index()\n#     X.loc[:, 'mkt_MACD_24_12'] = X['mkt_close_M12'] - X['mkt_close_M24']\n#     X[['MACD_12_24', 'MACD_12_24_sign', 'MACD_12_24_diff']] = roll_mkt.apply(macd, n_fast=12, n_slow=24)[['MACD_', 'MACDsign_', 'MACDdiff_']]\n#     X[['MACD_3_6', 'MACD_3_6_sign', 'MACD_3_6_diff']] = roll_mkt.apply(macd, n_fast=3, n_slow=6)[['MACD_', 'MACDsign_', 'MACDdiff_']]\n#     X.loc[:, 'volume_diff'] = roll_mkt[['volume']].pct_change().sort_index()\n    #X.loc[:, 'volume_MACD_6_3'] = roll_mkt[['volume']].pct_change().rolling(3).mean() - roll_mkt[['volume']].pct_change().rolling(6).mean()\n#     X['TSI'] = roll_mkt.apply(true_strength_index, r=6, s=12)[['TSI_']]\n#     X['RSI'] = roll_mkt.apply(RSI, n=15)[['RSI_']]\n#     X['TRIX'] = roll_mkt.apply(trix, n=10)[['Trix_']]\n#     X[['OBV']] = roll_mkt.apply(on_balance_volume, n=12)[['OBV_']]\n#     X['shift_10_1'] = roll_mkt.apply(shift_10)[['shift_10_1']]\n#     X['SOd'] = roll_mkt.apply(stochastic_oscillator_d, n=6)[['SOd']]\n#     X['std_10'] = roll_mkt.apply(std_rol, n=10)[['std']]\n#     X['KST'] = roll_mkt.apply(kst_oscillator)[['KST']]\n#     X['Copp'] = roll_mkt.apply(coppock_curve)[['Copp']]\n#     X['std_20'] = roll_mkt.apply(std_rol, n=20)[['std']]\n#     X['std_diff'] = X['std_10'] - X['std_20']\n#     X['ewma'] = roll_mkt.apply(ewma, n=10)[['ewma']]\n#     X['shift_10_2'] = roll_mkt.apply(shift_20)[['shift_10_2']]\n#     X.loc[:, 'shift_10_3'] = roll_mkt.apply(shift_30)[['shift_10_3']]\n#     del roll_mkt\n    \n    #roll_news = X[['assetCode', 'sentimentPositive_mean', 'sentimentNegative_mean']].groupby('assetCode')\n    #X['pos_ret'] = roll_news[['sentimentPositive_mean']].diff()\n    #X['neg_ret'] = roll_news[['sentimentNegative_mean']].diff()\n    #X['diff'] = X.returnsOpenPrevRaw10 - X.returnsOpenPrevMktres10\n    \n    #X['pos_MACD'] = roll_news[['sentimentPositive_mean']].rolling(3).mean().reset_index(0,drop=True).sort_index() - roll_news[['sentimentPositive_mean']].rolling(6).mean().reset_index(0,drop=True).sort_index()\n    #X['neg_MACD'] = roll_news[['sentimentNegative_mean']].rolling(3).mean().reset_index(0,drop=True).sort_index() - roll_news[['sentimentNegative_mean']].rolling(6).mean().reset_index(0,drop=True).sort_index()\n    \n    #X['pos_range'] = X.sentimentPositive_max - X.sentimentPositive_min\n    #X['neg_range'] = X.sentimentNegative_max - X.sentimentNegative_min\n    #del roll_news\n    \n    #X = X[['returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'diff_open_close', 'vol_diff', 'return_close_M6']]\n#     X = X[['returnsClosePrevRaw1', 'returnsClosePrevMktres1', 'returnsClosePrevRaw10', 'returnsClosePrevMktres10', 'volume_diff', \n#            'mkt_MACD_6_3', 'volume_MACD_6_3', 'sentimentPositive_mean', 'sentimentPositive_max', \n#            'sentimentNegative_mean','sentimentNegative_max', 'volumeCounts24H_mean',\n#            'TSI', 'shift_10_1']]\n    \n    #X = X[['returnsOpenPrevMktres10', 'shift_10_1']]\n#     X['neg'] = X['sentimentNegative_median']*X['noveltyCount12H_median']\n#     X['pos'] = X['sentimentPositive_median']*X['noveltyCount12H_median']\n    \n    X = X.loc[:,['time', 'assetCode', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsOpenPrevRaw10', 'open', 'close', 'volume']]\n#     X = X.loc[:,['time', 'assetCode', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsOpenPrevRaw10', 'noveltyCount12H_median', 'sentimentNegative_median', 'sentimentPositive_median']]\n    \n    roll_mkt = X.loc[:,['assetCode', 'returnsOpenPrevMktres1', 'returnsOpenPrevMktres10']].groupby('assetCode')\n    X['MACD_3_6_diff'] = roll_mkt.apply(macd_avg, n_fast=3, n_slow=6)[['MACDdiff_']]\n    X['shift_10_1'] = roll_mkt.apply(shift_10)[['shift_10_1']]\n    \n#     X['TSI'] = roll_mkt.apply(true_strength_index, r=6, s=12)[['TSI_']]\n#     X['RSI'] = roll_mkt.apply(RSI, n=15)[['RSI_']]\n#     X['TRIX'] = roll_mkt.apply(trix, n=10)[['Trix_']]\n#     X[['OBV']] = roll_mkt.apply(on_balance_volume, n=12)[['OBV_']]\n#     X['SOd'] = roll_mkt.apply(stochastic_oscillator_d, n=6)[['SOd']]\n#     X['std_10'] = roll_mkt.apply(std_rol, n=10)[['std']]\n#     X['KST'] = roll_mkt.apply(kst_oscillator)[['KST']]\n#     X['Copp'] = roll_mkt.apply(coppock_curve)[['Copp']]\n#     X['ewma'] = roll_mkt.apply(ewma, n=20)[['ewma']]\n\n    del roll_mkt\n    \n    X['temp'] = X['returnsOpenPrevRaw10'] * np.sqrt(X['volume'])\n    dico = X[['time', 'temp']].groupby('time').mean().to_dict()\n    X['mkt_vol'] = X.time.map(dico['temp'])\n    \n    dico = X[['time', 'returnsOpenPrevMktres1']].groupby('time').mean().to_dict()\n    X['mkt_1'] = X.time.map(dico['returnsOpenPrevMktres1'])\n    \n    dico = X[['time', 'returnsOpenPrevMktres10']].groupby('time').mean().to_dict()\n    X['mkt_10'] = X.time.map(dico['returnsOpenPrevMktres10'])\n    \n#     dico = X[['time', 'returnsOpenPrevMktres10']].groupby('time').std().to_dict()\n#     X['mkt_10_std'] = X.time.map(dico['returnsOpenPrevMktres10'])\n\n    X['daily_r'] = (X['close']  - X['open'] )/X['open']\n    \n    X = X.loc[:, ['time', 'shift_10_1', 'returnsOpenPrevMktres10', 'MACD_3_6_diff', 'returnsOpenPrevRaw10', 'assetCode']]\n    \n#     X = X.loc[:, ['time', 'shift_10_1', 'returnsOpenPrevMktres10', 'MACD_3_6_diff', 'returnsOpenPrevRaw10', 'assetCode',\n#                  'TSI', 'RSI', 'TRIX', 'OBV', 'SOd', 'std_10', 'KST', 'Copp', 'ewma', 'mkt_1', 'mkt_10', 'mkt_vol']]\n    \n#     X = X.loc[:, ['time', 'assetCode', 'SOd']]\n#     X = X.loc[:, ['time', 'assetCode', 'mkt_1', 'mkt_10', 'mkt_vol', 'daily_r']]\n\n\n    #X = X[['shift_10_1', 'returnsOpenPrevMktres10', 'MACD_3_6_diff', 'assetCode', 'diff']]\n    X = X.replace([np.inf, -np.inf], np.nan)\n    if clusters is not None:\n        #X = pd.concat([X, pd.get_dummies(X['assetCode'].map(correl_cluster).fillna(100))], axis=1)\n        volatility = X.assetCode.map(clusters)\n        X['volatility'] = np.array(volatility.fillna(volatility.median()))\n        #X = pd.concat([X, X['assetCode'].map(correl_cluster)], axis=1)\n\n    return(X)\n\ndef add_features_news(X, clusters=None):\n    X = X.loc[:,['time', 'assetCode', 'sentimentNegative_min',\n       'sentimentNegative_max', 'sentimentNegative_mean',\n       'sentimentNegative_std', 'sentimentPositive_min',\n       'sentimentPositive_max', 'sentimentPositive_mean',\n       'sentimentPositive_std', 'noveltyCount12H_min', 'noveltyCount12H_max',\n       'noveltyCount12H_mean', 'noveltyCount12H_std', 'volumeCounts12H_min',\n       'volumeCounts12H_max', 'volumeCounts12H_mean', 'volumeCounts12H_std']]\n    return(X)\n\n\n\ndef add_features2(X, clusters=None):\n    \n\n    X = X.loc[:,['assetCode', 'returnsOpenPrevMktres1', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsOpenPrevRaw10']]\n    \n    roll_mkt = X.loc[:,['assetCode', 'returnsOpenPrevMktres1', 'returnsOpenPrevMktres10']].groupby('assetCode')\n    X.loc[:,'MACD_3_6_diff'] = np.array(roll_mkt.apply(macd_avg, n_fast=3, n_slow=6))#[['MACDdiff_']]\n\n    X.loc[:,'shift_10_1'] = np.array(roll_mkt.apply(shift_10))#[['shift_10_1']]\n\n    del roll_mkt\n    \n    \n    \n    #X = X[['returnsOpenPrevMktres10', 'shift_10_1']]\n    X = X[['shift_10_1', 'returnsOpenPrevMktres10', 'MACD_3_6_diff', 'returnsOpenPrevRaw10', 'assetCode']]\n    #X = X[['shift_10_1', 'returnsOpenPrevMktres10', 'MACD_3_6_diff', 'assetCode', 'diff']]\n    \n    X = X.replace([np.inf, -np.inf], np.nan)\n    \n    if clusters is not None:\n        #X = pd.concat([X, pd.get_dummies(X['assetCode'].map(correl_cluster).fillna(100))], axis=1)\n        volatility = X['assetCode'].map(clusters)\n#         volatility = np.array(volatility.fillna(volatility.median()))\n        X['volatility'] = np.array(volatility.fillna(volatility.median()))\n        #X = pd.concat([X, X['assetCode'].map(correl_cluster)], axis=1)\n\n    return(X)\n\n#asset_codes = X[['assetCode']]\nX_ = add_features(X)#, clusters=weights)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46e98a9d50a8554f06677e2581f2df600d628c67"
      },
      "cell_type": "code",
      "source": "sns.regplot(X_.daily_r.iloc[:100000], y.iloc[:100000])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "7b3c682e04b63dd384c8e0dd59e69632f0700ff5"
      },
      "cell_type": "code",
      "source": "X_.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e08c59579593104b21eb7626d83006922e95338b"
      },
      "cell_type": "code",
      "source": "#X = X.replace([np.inf, -np.inf], np.nan)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "923e003a58a03088e2747eb8894920e77fef364e",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "X_.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc2683b1a92c5b386c0e47249674c0747cd0214f",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "X_.corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93742288299a11315c2ed3d0f723b9e6c260c9c3"
      },
      "cell_type": "code",
      "source": "import seaborn as sns\n\n# sns.regplot(X_.KST[:80000].clip(-20000, 20000), y[:80000])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38f2ea675409d8385a3c419440f852bd7e8fa093"
      },
      "cell_type": "code",
      "source": "n_train = int(X_.shape[0] * 0.8)\nX_train, y_train = X_.iloc[:n_train], y.iloc[:n_train]\n#X_train, y_train = X_train.loc[X_train.index & u_valid], y_train.loc[y_train.index & u_valid]\nX_valid, y_valid = X_.iloc[n_train:], y.iloc[n_train:]\n#X_valid, y_valid = X_valid.loc[X_valid.index & u_valid], y_valid.loc[y_valid.index & u_valid]\n\n\ny_train_size = np.abs(y_train)\ny_valid_size = np.abs(y_valid)\n\ny_train = y_train >= 0\ny_valid = y_valid >= 0\n\n# y_train = y_train/X_train.volatility\n# y_valid = y_valid/X_valid.volatility\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf4881ed13fc3a23f973588b496127092ae8b5d4"
      },
      "cell_type": "code",
      "source": "assets_to_drop = X_train.groupby('assetCode')[['returnsOpenPrevMktres10']].last()\nassets_to_drop = list(assets_to_drop[np.isnan(assets_to_drop.returnsOpenPrevMktres10)].index)\n\nX_train = X_train[~X_train.assetCode.isin(assets_to_drop)]\ny_train = y_train.loc[X_train.index]\ny_train_size = y_train_size.loc[X_train.index]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3411c8c915483366d3ca5a1a688a2a743fd3c06b"
      },
      "cell_type": "code",
      "source": "X_train = X_train.dropna()\n\ntrain_assets = X_train.assetCode\nvalid_assets = X_valid.assetCode\n\n# train_time = X_train.time.apply(np.datetime64)\n# valid_time = X_valid.time.apply(np.datetime64)\n\ntrain_time = X_train.time\nvalid_time = X_valid.time\n\nX_train.drop('assetCode', axis=1, inplace=True)\nX_valid.drop('assetCode', axis=1, inplace=True)\nX_train.drop('time', axis=1, inplace=True)\nX_valid.drop('time', axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7448178810a25ccde164027839f03a136a26a58e"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69ef820ec85114ba0e030dc1d85f2d8bccdc1c40"
      },
      "cell_type": "code",
      "source": "X_valid.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3459b71ca2fb407109d31d73abf5d1cb55976d2a"
      },
      "cell_type": "code",
      "source": "replace_val = X_train.median()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea4b56f8bf063c0e03099c9cf2876599a4bb0515"
      },
      "cell_type": "code",
      "source": "X_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fdc20d8095dab5adbf753ae9cfa8149b2a1fefbf"
      },
      "cell_type": "code",
      "source": "X_train = X_train.dropna()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1a19c18fb39e32f5b5690cb51b15e2a7f37bb83"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac6a0f2a2760da7ab83881e61530085f96175cff",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "for col in X_train.columns:\n    X_train[col] = X_train[col].fillna(replace_val[col])\n    X_valid[col] = X_valid[col].fillna(replace_val[col])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5dd4ffab250e7b6d4aa520a4c08e79a3fa2be1e"
      },
      "cell_type": "code",
      "source": "#X_train = X_train.dropna()\ny_train = y_train.loc[X_train.index]\ny_train_size = y_train_size.loc[X_train.index]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b4641df2b07d5f45617568d77c030a0904376b6"
      },
      "cell_type": "code",
      "source": "#X_train.drop(['assetCode', 'dayofweek', 'month'], axis=1, inplace=True)\n#X_valid.drop(['assetCode', 'dayofweek', 'month'], axis=1, inplace=True)\n\nstd = StandardScaler()\nstd.fit(X_train)\nX_train = pd.DataFrame(std.transform(X_train), index = X_train.index, columns=X_train.columns)\nX_valid = pd.DataFrame(std.transform(X_valid), index = X_valid.index, columns=X_valid.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f7eae03af77f7e5714da5ab297bffcceeb8771e"
      },
      "cell_type": "code",
      "source": "X_train['time'] = train_time\nX_valid['time'] = valid_time\n\nX_train['assetCode'] = train_assets\nX_valid['assetCode'] = valid_assets",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96c07816ec8c005e01d1a607f034c1af4ada114f"
      },
      "cell_type": "code",
      "source": "def sigma_score(preds, valid_data, t_valid):\n    df_time = t_valid.factorize()[0]#valid_data.params['extra_time']\n    labels = valid_data#.get_label()\n    \n#    assert len(labels) == len(df_time)\n\n    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n    \n    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n    # is a pd.Series and call `group_by`\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() / x_t_sum.std()\n\n    return 'sigma_score', score, True",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b8180c106fca6cd101381874685282f4e794e114"
      },
      "cell_type": "code",
      "source": "from scipy.optimize import minimize\n\ndef information_ratio_2(w, y_hat, sigma):\n#     r = w[0]*y_hat[0] + w[1]*y_hat[1]\n    r = np.dot(w.T, y_hat)\n#     s = np.sqrt(w[0]*w[0]*epsilon[0]*epsilon[0] + w[1]*w[1]*epsilon[1]*epsilon[1])\n    s = np.sqrt(np.dot(w.T, np.dot(sigma, w)))\n    #print(np.dot(w.T, np.dot(sigma, w)))\n    return -r/s\n\ndef compute_w(model, x, covar):\n    #cov = covar.loc[x.assetCode, x.assetCode]\n    cov = covar.reindex(index = x.assetCode, columns = x.assetCode)\n    avg_diag = np.mean([covar.iloc[i, i] for i in range(len(covar))])\n    for k in range(len(cov)):\n        if np.isnan(cov.iloc[k, k]):\n            cov.iloc[k, k] = avg_diag\n    cov = cov.fillna(0)\n    print(cov.isnull().sum().sum())\n    bounds = [(0.2,1) for i in range(x.shape[0])]\n    x = x.drop(['time', 'assetCode'], axis=1)\n    pred = model.predict(x)-0.5\n    res = minimize(\n        information_ratio_2,\n        np.array([0.6 for i in range(x.shape[0])]),\n        args=(pred, cov),\n        bounds=bounds, \n        method='L-BFGS-B',\n        options = {'maxfun':100}\n    )\n    print(res.status)\n    return(pred*res.x)\n\ndef compute_w2(model, x, covar):\n    #cov = covar.loc[x.assetCode, x.assetCode]\n    cov = covar.reindex(index = x.assetCode, columns = x.assetCode)\n    avg_diag = np.mean([covar.iloc[i, i] for i in range(len(covar))])\n    for k in range(len(cov)):\n        if np.isnan(cov.iloc[k, k]):\n            cov.iloc[k, k] = avg_diag\n    cov = cov.fillna(0)\n    print(cov.isnull().sum().sum())\n    bounds = [(0.2,1) for i in range(x.shape[0])]\n    x = x.drop(['time', 'assetCode'], axis=1)\n    pred = model.predict(x)-0.5\n    covar_inv = np.linalg.inv(cov)\n    res = np.dot(covar_inv, pred)/np.dot(np.array([1 for i in range(len(pred))]), np.dot(covar_inv, pred))\n    print(information_ratio_2(res, pred, cov))\n    return(pred*res)\n    \ndef new_pred(model, x, covar):\n    result = np.array([])\n    for t in x.time.unique():\n        print(t)\n        result = np.concatenate((result, compute_w(model, x[x.time == t], covar)))\n    return(result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9eb96f35d4587143d2259eed2a58d2305d9b0c6a"
      },
      "cell_type": "code",
      "source": "#model = XGBClassifier(n_jobs=4, n_estimators=40, max_depth=8, eta=0.1, subsample=0.9, colsample_bytree = 0.9, rate_drop=0.05, booster='dart')\n#model.fit(X_train, y_train)\n\n#from xgboost import plot_importance\n#plot_importance(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c66f7fc51735a52aa9f6de368509f35e7133496"
      },
      "cell_type": "code",
      "source": "#from sklearn.feature_selection import mutual_info_classif, f_classif\n#mutual_info_classif(X_train, y_train, n_neighbors = 100)\n#f_classif(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7be28537e18820b8511062b2da262dba9addccd8"
      },
      "cell_type": "code",
      "source": "model = XGBRegressor(n_jobs=4, n_estimators=32, max_depth=8, min_child_weight=1, eta=0.1, subsample=0.6, rate_drop=0.05, booster='dart', gamma=0.05)\n\n#model = SGDRegressor(max_iter=60)\n\nmodel.fit(X_train.drop(['time', 'assetCode'], axis=1), y_train)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "eb67e42a291347eceaf4a394316539c0f4e736d5"
      },
      "cell_type": "code",
      "source": "model.score(X_train.drop(['time', 'assetCode'], axis=1), y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "490228a1b6e3be4509a24ad566f8235979b1be16"
      },
      "cell_type": "code",
      "source": "model.score(X_valid.drop(['time', 'assetCode'], axis=1), y_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86fc3475068def236bf7595a71bea3e206b99914"
      },
      "cell_type": "code",
      "source": "sns.regplot(y_valid[100000:200000], model.predict(X_valid.drop(['time', 'assetCode'], axis=1))[100000:200000])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0051ec8558aec09b56bfcb82e5cf0ed9209780a2"
      },
      "cell_type": "code",
      "source": "# model_size.coef_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "2350a5d2f58ff308431c97533b0908615573843c"
      },
      "cell_type": "code",
      "source": "# sns.regplot(model_size.predict(X_valid)[:50000], y_valid_size[:50000])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1eb86b09aa9b0e9894e7b5871c660fe91957638d"
      },
      "cell_type": "code",
      "source": "from catboost import CatBoostClassifier\nimport time\n\nmodel = CatBoostClassifier(thread_count=4,                            \n                            #n_estimators=400, \n                            max_depth=10, \n                            eta=0.1, \n                            loss_function='Logloss', \n                            random_seed = 64738,\n                            iterations=100,\n                            verbose=10)\nmodel.fit(X_train.drop(['time'], axis=1), y_train, cat_features=[3])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "996c09991797b476f99ae4c9ea364ecf50980491"
      },
      "cell_type": "code",
      "source": "model.get_feature_importance()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba60abd0cf3d854a7dfb60abe87c151c65bba586"
      },
      "cell_type": "code",
      "source": "X_train.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9697b3912266a03cd329138c6d8a14b83fa970e3",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier\n# model4 = GaussianNB()\n# model1 = XGBClassifier(n_jobs=4, n_estimators=40, max_depth=10, eta=0.1, subsample=0.9, colsample_bytree = 0.9, rate_drop=0.05, booster='dart')\nmodel3 = XGBClassifier(n_jobs=4, n_estimators=64, max_depth=11, eta=0.1, subsample=0.5, rate_drop=0.05, booster='dart', verbose=True)\n# model4 = KNeighborsClassifier(n_neighbors=50, n_jobs=4)\nmodel1 = XGBClassifier(n_jobs=4, n_estimators=16, max_depth=11, min_child_weight=1, eta=0.1, subsample=0.6, rate_drop=0.05, booster='dart', gamma=0.05)#, eval_metric='auc')\nmodel2 = SGDClassifier(loss='modified_huber', penalty='l1', alpha=0.001, max_iter=30, verbose=10)\n#model2 = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=0.0001, fit_intercept=True, random_state=3, max_iter=400, verbose=0, warm_start=False, n_jobs=1)\n# model3 = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=0.0005, fit_intercept=True, random_state=3, max_iter=400, verbose=0, warm_start=False, n_jobs=1)\n\n\nmodel = VotingClassifier([('nb', model1), ('l1', model2)], voting='hard')\n#model=KNeighborsClassifier()\nmodel3 = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=0.01, fit_intercept=True, random_state=3, max_iter=800, verbose=0, warm_start=False, n_jobs=1)\n#model = XGBClassifier(n_jobs=4, n_estimators=8, max_depth=10, eta=0.1, subsample=0.9, colsample_bytree = 0.9, rate_drop=0.05, booster='dart', verbose=10)\nmodel = XGBClassifier(n_jobs=4, n_estimators=16, max_depth=11, min_child_weight=1, eta=0.08, subsample=0.6, rate_drop=0.05, booster='dart', gamma=0.05)#, eval_metric='auc')\nclf=RandomForestClassifier(n_estimators=4, n_jobs=1, criterion= 'entropy', bootstrap=False, class_weight='balanced_subsample')\nclf=XGBClassifier(n_jobs=4, n_estimators=4, max_depth=11, eta=0.1, subsample=1, rate_drop=0.0, booster='dart', verbose=True)\nclf=SGDClassifier(loss='modified_huber', penalty='l1', alpha=0.001)\n#model = BaggingClassifier(base_estimator=clf, n_estimators=40, max_samples=0.5, n_jobs=4, verbose=10)\n#model3.fit(X_train, y_train)\nmodel2=KNeighborsClassifier(n_neighbors=20)\nmodel=XGBClassifier(n_jobs=4, n_estimators=16, max_depth=11, eta=0.1, subsample=0.5, rate_drop=0.05, booster='dart', verbose=True)\n# model = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=0.01, fit_intercept=True, random_state=3, max_iter=1000, verbose=0, warm_start=False, n_jobs=1)\n#model = SGDClassifier(loss='modified_huber', penalty='l1', alpha=0.001, max_iter=30, verbose=10)\n#model=RandomForestClassifier(n_estimators=32, max_depth=7, n_jobs=4, criterion= 'entropy', bootstrap=False, class_weight='balanced_subsample', verbose=10)\n#model = VotingClassifier([('nb', model1), ('l1', model3)], voting='hard')\nmodel.fit(X_train.drop(['time', 'assetCode'], axis=1), y_train)\n#model=KNeighborsClassifier(n_neighbors=40)\n#model.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a98ec5f330c1a4ed6cb65762009283f00eab9aed",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from xgboost import plot_importance\nplot_importance(model)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "e3172b74cedf9c9ada1d51dc0c64e1fed058a744"
      },
      "cell_type": "code",
      "source": "np.unique(X_valid.time)[0]#.tz_localize(None)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "300556e607fc6d8db98fcbd2ee655bc73f58a3aa"
      },
      "cell_type": "code",
      "source": "model.coef_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "907913fdeac5301a5e9571860189c2c2ec0772c7"
      },
      "cell_type": "code",
      "source": "#pred = new_pred(model, X_valid, covar)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "53897bf0c1ea277b3625b21f6ab4c4ec8a27f0f7"
      },
      "cell_type": "code",
      "source": "# df_result = np.array([x[1] for x in model.predict_proba(X_valid)])\n# for i in range(len(df_result)):\n#     if np.abs(df_result[i] - 0.5) < 0.01:\n#         df_result[i] = 0\n#     else:\n#         df_result[i] = df_result[i] - 0.5\n# df_result = (model.predict(X_valid)-0.5)*df_result\n#df_result = (model.predict(X_valid)-0.5 + model3.predict(X_valid)-0.5)\ndf_result = (model.predict(X_valid.drop(['time', 'assetCode'], axis=1))-0.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "711402efc0d91e20c6f49de525aa468fb8eed77f"
      },
      "cell_type": "code",
      "source": "pred1 = model.predict(X_valid.drop(['time', 'assetCode'], axis=1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2aed4cb309d01afd9a7e118837226c793c8b471a"
      },
      "cell_type": "code",
      "source": "pred2 = model.predict(X_valid.drop(['time', 'assetCode'], axis=1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1068be511d19934e6993eee90ba6b38356222bcd"
      },
      "cell_type": "code",
      "source": "np.(pred1-0.5)-(pred2-0.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92e5f51d98251db1874fbde008daf635495a9ed7"
      },
      "cell_type": "code",
      "source": "sns.distplot((pred1-0.5)-(pred2-0.5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a3d618de6d5ec77deaa4def80bdd9db810a91a4"
      },
      "cell_type": "code",
      "source": "df_result_train = X_train.returnsOpenPrevMktres10 >= 0\ndf_result_valid = X_valid.returnsOpenPrevMktres10 >= 0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "375ac77a02499b7cf2022e1c924760308f1a75a6"
      },
      "cell_type": "markdown",
      "source": "# Accuracy"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b98482047ad1b8256f51fdb5e6bfc77ddaaca265"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(df_result_train,y_train))\nprint(accuracy_score(df_result_valid,y_valid))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ea742dedc07573ce1afa693055cbe7c2ed03a29"
      },
      "cell_type": "code",
      "source": "print(accuracy_score(df_result_valid.iloc[good_universe_index],y_valid.iloc[good_universe_index]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1b2babf8bc3cd3e737dfbd85570c67cab43558e",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(model.predict(X_train.drop(['time', 'assetCode'], axis=1)),y_train))\nprint(accuracy_score(model.predict(X_valid.drop(['time', 'assetCode'], axis=1)),y_valid))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "afc1191704bae4feb3f65db50c06ccff919ce8a3",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print(accuracy_score(model.predict(X_valid.drop(['time', 'assetCode'], axis=1))[good_universe_index],y_valid.iloc[good_universe_index]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "66a409f34ae8e41325e550dba6a80e25b90a745b"
      },
      "cell_type": "markdown",
      "source": "# Sigma Score"
    },
    {
      "metadata": {
        "_uuid": "ab3f2b32e20ff06243c0fd94b5b4780cbf88f640"
      },
      "cell_type": "markdown",
      "source": "## Signs"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b593edeeffffb7716060818b46649740d0035cfe",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "sigma_score(df_result_valid-0.5, y_valid-0.5, t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "b6022d18ed3e3a40d7b7bb473167e8b7dc4dacd5"
      },
      "cell_type": "code",
      "source": "sigma_score((df_result_valid-0.5).iloc[good_universe_index], (y_valid-0.5).iloc[good_universe_index], t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6046bc8d3d39e487334a071c3f1238284a8462e0"
      },
      "cell_type": "code",
      "source": "sigma_score(df_result, y_valid-0.5, t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "2da7eaea7d17c0c4e09c5fd057580f2e576da4fe"
      },
      "cell_type": "code",
      "source": "sigma_score(df_result[good_universe_index], (y_valid-0.5).iloc[good_universe_index], t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e19a26e71890bd78a17a3995dd953206bc338548"
      },
      "cell_type": "code",
      "source": "sigma_score([x[1] - 0.5 for x in model.predict_proba(X_valid.drop(['time', 'assetCode'], axis=1))], y_valid-0.5, t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "625b761c9aa746165c7175beee68c6328eb91dfa"
      },
      "cell_type": "code",
      "source": "sigma_score([x[1] - 0.5 for x in model.predict_proba(X_valid.drop(['time', 'assetCode'], axis=1))[good_universe_index]], (y_valid-0.5).iloc[good_universe_index], t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89230174bb67bb8f305a6533c0ed7ed9cabf748d"
      },
      "cell_type": "markdown",
      "source": "## Test of weighting schemes"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2975f8bf826f8c522f0dbe25fbb13db09a9108f0"
      },
      "cell_type": "code",
      "source": "#return_train = y.iloc[:n_train]\n#m = min(return_train[return_train > 0].min(), return_train[return_train < 0].max() * -1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "bea08a5e561bba0006f83a6c8ad7bbd01001a4ba"
      },
      "cell_type": "code",
      "source": "min(df_result/model_size.predict(X_valid)**8)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83f4c5bb57e77665c84e096716317a333207052e"
      },
      "cell_type": "code",
      "source": "#sigma_score(df_result*(1/X_valid.volatility)*m, y_valid-0.5, t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d474c50709e1157ceac80a89a50561e55ef362ed",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "sigma_score(df_result, y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "8642660cb75c3196f9bbe70d93e2edae6d141a10"
      },
      "cell_type": "code",
      "source": "sigma_score(0.0000001*df_result/model_size.predict(X_valid)**8, y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9560197bcce2f729e22c290c5398672dc1d5f7e"
      },
      "cell_type": "code",
      "source": "pred = (model.predict(X_valid) - 0.5)/((0.01+model_size.predict(X_valid))**8)\nsigma_score(np.abs([x[1]-0.5 for x in model.predict_proba(X_valid)])**(0.8)*pred/np.max(np.abs(pred)), y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b76f881d19376127992a2356f11c6b2a097c4acf"
      },
      "cell_type": "code",
      "source": "sigma_score(df_result, y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a97ee4b137f65480ee0a591017d439b6b943acf4"
      },
      "cell_type": "code",
      "source": "sigma_score(np.array([x[1] - 0.5 for x in model.predict_proba(X_valid)]), y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae211ee58ae6ec16802fb79918a58c4c56b8cf89"
      },
      "cell_type": "code",
      "source": "sigma_score(np.array([np.sign(x[1] - 0.5) if np.abs(x[1] - 0.5)>0.03 else 0 for x in model.predict_proba(X_valid)]), y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c628aef073331c644b133410179a819c30aff74d"
      },
      "cell_type": "code",
      "source": "#sigma_score((np.array([x[1] - 0.5 for x in model.predict_proba(X_valid)]) * (X_valid.volatility)**(1/20))[good_universe_index], y.iloc[n_train:].iloc[good_universe_index], t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5255dd29c893a10583acbdd17e01f869f29d647b"
      },
      "cell_type": "markdown",
      "source": "## Returns"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "359034512bc6befae5d706778a19fab568f54f5a"
      },
      "cell_type": "code",
      "source": "sigma_score(df_result_valid-0.5, y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fe39e094dcb3e2a15db8c4d8144535c5b3806e5"
      },
      "cell_type": "code",
      "source": "sigma_score((df_result_valid-0.5)[good_universe_index], y.iloc[n_train:].iloc[good_universe_index], t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6532b730f0a4be5620ed20edd87cbf1e2144a5e"
      },
      "cell_type": "code",
      "source": "sigma_score(df_result, y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d09c14e6ceb47eff806b85ba2eb33b674bec52ab"
      },
      "cell_type": "code",
      "source": "sigma_score(df_result[good_universe_index], y.iloc[n_train:].iloc[good_universe_index], t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "43fd49c3b74c902c4b7163d0ad1a2b53968d740c"
      },
      "cell_type": "code",
      "source": "sigma_score([(x[1] - 0.5) for x in model.predict_proba(X_valid.drop(['time', 'assetCode'], axis=1))], y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49621146a980723c429271baec8bc5d58e4d1d8b"
      },
      "cell_type": "code",
      "source": "sigma_score([x[1] - 0.5 for x in model.predict_proba(X_valid.drop(['time', 'assetCode'], axis=1))[good_universe_index]], y.iloc[n_train:].iloc[good_universe_index], t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d085a5fa9e3aa76f0ac954a888d2b7088b9b7e1b"
      },
      "cell_type": "code",
      "source": "sigma_score(np.array([np.sign(x[1] - 0.5) if np.abs(x[1] - 0.5)>0.02 else np.sign(x[1] - 0.5)/10 for x in model.predict_proba(X_valid.drop(['time', 'assetCode'], axis=1))])[good_universe_index], y.iloc[n_train:].iloc[good_universe_index], t_valid_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55df2935c585e53413c6ba74a1101d5a7b5b868b"
      },
      "cell_type": "code",
      "source": "# import seaborn as sns\nsigma_score(pred2-0.5 + 0.04*(pred1-0.5), y.iloc[n_train:], t_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3988b7c20a48bdd78bf0a7372bf669bea72451ac"
      },
      "cell_type": "code",
      "source": "# for i in range(len(model.coef_[0])):\n#     print(X_train.columns[i], model.coef_[0][i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a19979aa4a3e04724176c90c5330d2b6117cfc7"
      },
      "cell_type": "markdown",
      "source": "evals_result = {}\n#m = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n#              early_stopping_rounds=100, feval=sigma_score, evals_result=evals_result)\n# model1 = SGDRegressor(loss='huber', penalty='l1', alpha=0.00006, l1_ratio=0.15, fit_intercept=True, max_iter=None, \n#                      tol=None, shuffle=True, verbose=0, epsilon=0.1, random_state=3, learning_rate='invscaling', \n#                      eta0=0.01, power_t=0.25, \n#                      warm_start=False, average=False, n_iter=None)\n\n# model2 = SGDRegressor(loss='epsilon_insensitive', penalty='l2', alpha=0.00006, l1_ratio=0.15, fit_intercept=True, max_iter=None, \n#                      tol=None, shuffle=True, verbose=0, epsilon=0.1, random_state=3, learning_rate='optimal', \n#                      eta0=0.01, power_t=0.25, \n#                      warm_start=True, average=False, n_iter=None)\n\n# model3 = SGDRegressor(loss='squared_epsilon_insensitive', penalty='l1', alpha=0.00006, l1_ratio=0.15, fit_intercept=True, max_iter=None, \n#                      tol=None, shuffle=True, verbose=0, epsilon=0.1, random_state=3, learning_rate='optimal', \n#                      eta0=0.01, power_t=0.25, \n#                      warm_start=False, average=False, n_iter=None)\n\n# model = XGBRegressor(silent=False, base_score=0, max_depth=9, learning_rate=0.1, \n#                       n_estimators=40, nthread=3, gamma=0.2,\n#                      min_child_weight=1, subsample=0.9, colsample_bytree = 0.9, rate_drop=0.05, booster='dart')\n\naugmented_X_train = pd.concat([X_train, asset_codes.loc[X_train.index]], axis =1)\nassets = set(asset_codes.assetCode)\nmodels = {a:LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=0.1, fit_intercept=True, \n                               random_state=3, max_iter=800, verbose=0, warm_start=False, n_jobs=1) for a in assets}\n\nmodel = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=0.0001, fit_intercept=True, random_state=3, max_iter=800, verbose=0, warm_start=False, n_jobs=1)\nmodel.fit(X_train, y_train)\n\ndf_result = 0*y_valid.copy()\nfor a in assets:\n    print(a)\n    indices = [x for x in asset_codes[asset_codes.assetCode == a].index if x <= X_train.index[-1]]\n\n    indices_test = [x for x in asset_codes[asset_codes.assetCode == a].index if x >= X_valid.index[0]]\n    \n    if len(indices_test) > 0: \n        if len(indices) > 0:\n            new_X = X_train.loc[indices]\n            new_y = y_train.loc[indices]\n            try:\n                models[a].fit(new_X, new_y)\n\n                new_X_valid = X_valid.loc[indices_test]\n                df_result.loc[indices_test] = models[a].predict(new_X_valid)\n            except ValueError:\n                #print(np.unique(new_y))\n                #df_result.loc[indices_test] = np.unique(new_y)[0]\n                new_X_valid = X_valid.loc[indices_test]\n                df_result.loc[indices_test] = model.predict(new_X_valid)\n        else:\n            new_X_valid = X_valid.loc[indices_test]\n            df_result.loc[indices_test] = model.predict(new_X_valid)\n    \n\n\n# model1.fit(X_train, y_train)\n# model2.fit(X_train, y_train)\n# model3.fit(X_train, y_train)\n\n\n#df_result = pd.DataFrame(evals_result['valid'])\n# df_result = 0.4*model1.predict(X_valid) + 0.3*model2.predict(X_valid) + 0.3*model3.predict(X_valid)\n#df_result = 0.2*df_result + 0.8*model1.predict(X_valid)\n#df_result = model1.predict(X_valid)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "593c75b6efd6d293c811b2158fb4b04b3b517efd"
      },
      "cell_type": "markdown",
      "source": "models[1].coef_"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27d678cedf96a9b2397e2139196dee940149521d",
        "scrolled": true
      },
      "cell_type": "markdown",
      "source": "model1.score(X_train, y_train)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d2c02b9f85481d355265aa3a1c30003b7866559"
      },
      "cell_type": "markdown",
      "source": "cols = X.columns\nfor i in range(len(cols)):\n    print(cols[i])\n    print(model1.coef_[i])"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ebcc5530649ee3c15af56c9879494e5e0cf59c87"
      },
      "cell_type": "markdown",
      "source": "model1.score(X_valid, y_valid)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c59107c7f2893799c4a1560538adca9e790fd541"
      },
      "cell_type": "markdown",
      "source": "from sklearn.metrics import accuracy_score\nprint(accuracy_score(df_result,y_valid))"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2ee36280f7cfe876278d6b6909daca97ef0cdc1"
      },
      "cell_type": "markdown",
      "source": "sigma_score(df_result-0.5, y_valid-0.5, t_valid)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38a1ff2f34f1f95788adefa8b9fb2d3b39af8000"
      },
      "cell_type": "code",
      "source": "#dtrain_full = lgb.Dataset(X, y, feature_name=train_cols, categorical_feature=categorical_cols)\n#X = X.drop(['assetCode', 'dayofweek', 'month'], axis=1)\n\n\n\n#X_, y = X_.loc[u_valid], y.loc[u_valid]\nasset = X_.assetCode\ntim = X_.time\nX_.drop(['assetCode', 'time'], axis=1, inplace=True)\nreplace_val = X_.median()\nX_ = X_.dropna()\ny = y.loc[X_.index]\nfor col in X_.columns:\n    X_[col] = X_[col].fillna(replace_val[col])\n#X = X.dropna()\n\n\nind_X = X_.index\nstd = StandardScaler()\nstd.fit(X_)\nX=pd.DataFrame(std.transform(X_), index=X_.index, columns = X_.columns)\nX['assetCode'] = asset\nX['time'] = tim",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "520266ca9e39227caaf9b9b676361aea0af4303f",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "u_valid[-1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3d32ff72368205c3139bbef888321b3cd5c12a3"
      },
      "cell_type": "code",
      "source": "X.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8bbb54bd6c8b767a1cdb5afd6495d19a2f445612"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "2178e3198e75b37f6dd09e0cc89dfc39b3162d4d"
      },
      "cell_type": "code",
      "source": "y_size = np.abs(y.loc[y.index & u_valid])\ny = y >=0\n#model = XGBClassifier(n_jobs=4, n_estimators=100, max_depth=10, eta=0.1, subsample=0.9, colsample_bytree = 0.95, rate_drop=0.05, booster='dart')\n#model.fit(X.loc[u_valid], y.loc[u_valid])\n#model = XGBClassifier(n_jobs=2, n_estimators=128, max_depth=10, eta=0.1, subsample=0.5, rate_drop=0.05, booster='dart', verbose=True)\nmodel = XGBClassifier(n_jobs=4, n_estimators=64, max_depth=10, eta=0.1, subsample=0.5, rate_drop=0.05, booster='dart', verbose=True)\nmodel.fit(X.drop(['assetCode', 'time'], axis=1), y)\n\nmodel_size = XGBRegressor(n_jobs=1, n_estimators=1, max_depth=8, min_child_weight=1, eta=0.09, subsample=0.6, rate_drop=0.05, booster='dart', gamma=0.05)\n#model_size.fit(X.loc[X.index & u_valid], y_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bc94c36371bade77aa29d2d6c8c24fbacf6184e"
      },
      "cell_type": "code",
      "source": "#sns.regplot(model_size.predict(X)[:50000], y_size[:50000])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7ed4ecb99e9ada11c7713a78a586b23f77c1a82"
      },
      "cell_type": "code",
      "source": "def make_predictions(predictions_template_df, le, model, replace_val, std, x):\n#     print(x.isnull().sum().sum())\n#     for col in x.columns:\n#         x[col] = x[col].fillna(replace_val[col])\n#     x = pd.DataFrame(std.transform(x), columns = x.columns, index=x.index)\n    #predictions_template_df.confidenceValue = [np.sign(y[1] - 0.5) if np.abs(y[1] - 0.5)>0.05 else 0 for y in model.predict_proba(x)]\n    predictions = (model.predict(x) - 0.5)#/((0.01+model_size.predict(x)**2))\n    predictions_template_df.confidenceValue = predictions#*(0.01**2)\n    \ndef make_predictions2(predictions_template_df, le, model, replace_val, std, x, weight):\n    predictions = (model.predict(x) - 0.5) * weight\n    predictions_template_df.confidenceValue = predictions",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6979a3dd6e16d802f6ac73590cb597175df9ae6"
      },
      "cell_type": "code",
      "source": "def online_learn(model_list, weight_list):\n    pass\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1dc0b6806d8aa5c78f8b0a3256eb33218a98192d"
      },
      "cell_type": "code",
      "source": "days = env.get_prediction_days()\n\nonline_train = []\n#univ_t = []\nx_t = []\n# assets_l = []\ni=0\n# weight=1\n# score=0.5\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    i+=1\n#     if i % 200 == 199:\n#         model.fit(X.iloc[2000*(i//100):].drop(['time', 'assetCode'], axis=1), y.iloc[2000*(i//100):])\n        \n    x, _ = get_x(market_obs_df, news_obs_df, le)\n    #univ_t.append(market_obs_df[['assetCode', 'universe']].copy())\n    d = market_obs_df.time[0]\n    print(d)\n    del news_obs_df\n    del market_obs_df\n    last_ind = dynamic_X.index[-1]+1\n    x.index = [i+last_ind for i in x.index]\n    dynamic_X = pd.concat([dynamic_X, x]).iloc[-(15*len(x.index)):]\n#     dynamic_X = fast_concat([dynamic_X, x]).iloc[-(15*len(x.index)):]\n    x_temp = dynamic_X.copy()\n    x_temp = add_features(x_temp)#, clusters=weights)\n    #x = x_temp.iloc[-len(x.index):]\n    #x = x.drop(['assetCode', 'dayofweek', 'month'], axis=1)\n    \n    x = x_temp.iloc[-len(x.index):].copy()\n    y_online = x[['returnsOpenPrevMktres10', 'assetCode']].dropna().copy()\n    online_train.append(x.copy())\n    \n    x.drop(['assetCode', 'time'], axis=1, inplace=True)\n    print(x.isnull().sum().sum())\n    for col in x.columns:\n        x[col] = x[col].fillna(replace_val[col])\n    x = pd.DataFrame(std.transform(x), columns = x.columns, index=x.index)\n    \n    #update = np.exp(-score+0.5)\n#     update = 0.5-score\n#     weight = min(max(weight+update, 0.5), 1.5)\n    \n    make_predictions(predictions_template_df, le, model, replace_val, std, x)\n    env.predict(predictions_template_df)\n#         assets_l = assets_l[1:]\n    if len(online_train)>10:\n        x_ = online_train[0]\n        x_ = x_[x_.assetCode != -1]\n        x_ = x_[x_.assetCode.isin(y_online.assetCode)]\n        x_.sort_values('assetCode', inplace=True)\n#         print(x_.iloc[:30])\n#         univ = univ_t[0]\n#         univ = univ[univ.assetCode != -1]\n#         univ = univ[univ.assetCode.isin(y_online.assetCode)].sort_values('assetCode')\n        y_online = y_online[y_online.assetCode != -1]\n        y_online = y_online[y_online.assetCode.isin(x_.assetCode)]\n        y_online.sort_values('assetCode', inplace=True)\n#         print(y_online.iloc[:30])\n        y_online = y_online.returnsOpenPrevMktres10\n        y = pd.concat([y, y_online])\n        asset = x_['assetCode']\n        tim = x_['time']\n        x_.drop(['assetCode', 'time'], axis=1, inplace=True)\n    \n        for col in x_.columns:\n            x_[col] = x_[col].fillna(replace_val[col])\n        x_ = pd.DataFrame(std.transform(x_), columns = x_.columns, index=x_.index)\n        score = model.score(x_, y_online >= 0)\n        print(score)\n        \n        x_t.append(np.sum((model.predict(x_)-0.5)*y_online))\n        if len(x_t) >= 2 and d.year <= 2017 or (d.year <= 2018 and d.month<=5):\n            temp = np.array(x_t)\n            print(np.mean(temp)/np.std(temp))\n        x_['assetCode'] = asset\n        x_['time'] = tim\n        X = pd.concat([X, x_])\n#         model.fit(x_, y_online >= 0, xgb_model=model)\n#         model.fit(x_, y_online >= 0)\n#         sns.regplot(x.returnsOpenPrevMktres10, y_online)\n#         plt.show()\n        online_train = online_train[1:]\n        \n#         univ_t = univ_t[1:]\n        \n        \nprint('Done!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f620a0578f669238652436e28fc9de9ed66acd7"
      },
      "cell_type": "code",
      "source": "env.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1b1f559c1d1fce49f9ed320e19ecac7e8d21225"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(16, 8))\nplt.plot(x_t)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65633d5e935b63c2d49570fdfa75882dd23f35c7"
      },
      "cell_type": "code",
      "source": "plt.plot(np.cumsum(x_t))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c8dce03d379327d12b8737cf65fea6fb52f45a4"
      },
      "cell_type": "code",
      "source": "np.mean(x_t)/np.std(x_t)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2af2f87bee272cb9d2ec70fe91ead42f15c1d0ce"
      },
      "cell_type": "code",
      "source": "y_t=[np.mean(x_t[:i])/np.std(x_t[:i]) for i in range(len(x_t))]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5602314a59560d834d3031e2553a67ca0c66ddc6"
      },
      "cell_type": "code",
      "source": "plt.plot(y_t[10:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fe9daf754618fbae6b693aa7b67ff5fed88834c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}