{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "from pathlib import Path\n",
    "import json, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I = lambda x: x\n",
    "logavg = lambda x,y: math.expm1((math.log1p(x)+math.log1p(y))/2)\n",
    "keepSigFig = lambda n: lambda x: round(x, -int(math.floor(math.log10(abs(x)))) + (n - 1)) if x else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #!#!#!#!#!#!#!#!#!#!#! Save System #!#!#!#!#!#!#!#!#!#!#!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-0839842a4234>, line 124)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-0839842a4234>\"\u001b[0;36m, line \u001b[0;32m124\u001b[0m\n\u001b[0;31m    if keepClient = None:\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class IndexFileSystem():\n",
    "    def __init__(self, directory, key):\n",
    "        self.dir = directory if isinstance(directory, Path) else Path(directory)\n",
    "        self.dir.mkdir(exist_ok=True, parents=True)\n",
    "        assert isinstance(key, str), \"key must be string\"\n",
    "        self.key = key\n",
    "        self.io = O()\n",
    "\n",
    "    def iterIndices(self):\n",
    "        for dot_params in self.dir.glob('*.' + self.key):\n",
    "            yield int(dot_params.stem)\n",
    "\n",
    "    def getFilePath(self, *, i):\n",
    "        return self.dir / (str(i) + '.' + name)\n",
    "\n",
    "    @staticmethod\n",
    "    def readWrapper(read):\n",
    "        '''wraps io read operations to safely return None if file does not exist'''\n",
    "        @wraps(read)\n",
    "        def read_safely(*a, **k):\n",
    "            try:\n",
    "                return read(*a, **k)\n",
    "            except FileNotFoundError:\n",
    "                return None\n",
    "        return read_safely\n",
    "\n",
    "    def assignIO(self, name, *, read, write, format='custom'):\n",
    "        assert format in ['bytes', 'text', 'custom'], \"argument `format` must be one of 'bytes' or 'text'\"\n",
    "        if format in ['bytes', 'text']:\n",
    "            self.io[name] = O(\n",
    "                read = readWrapper( lambda *,i: read(getattr(self.getFilePath(name, i=i), 'read_'+format)()) ),\n",
    "                write = lambda x,*,i: getattr(self.getFilePath(name, i=i), 'write_'+format)(write(x))\n",
    "            )\n",
    "        elif format == 'custom':\n",
    "            def readCustom(*, i):\n",
    "                file = self.getFilePath(name, i=i)\n",
    "                if not file.exists():\n",
    "                    return None\n",
    "                try:\n",
    "                    return read(file)\n",
    "                except Exception:\n",
    "                    return read(str(file))\n",
    "            def writeCustom(*, i):\n",
    "                file = self.getFilePath(name, i=i)\n",
    "                try:\n",
    "                    write(x, file=file)\n",
    "                except Exception:\n",
    "                    write(x, file=str(file))\n",
    "            self.io[name] = O(read=readCustom, write=writeCustom)\n",
    "\n",
    "class IndexDataStore():    \n",
    "    def __init__(self, file, _factory_=False):\n",
    "        if not _factory_:\n",
    "            assert False, \"Cannot instantiate IndexDataStore normally. Please use factory static method.\"\n",
    "        self.file = file\n",
    "        self.op = O()\n",
    "        self.lists = O()\n",
    "        self.key = None\n",
    "        self.keyFunc = lambda keyVal, *, client=False: object()\n",
    "        self.tbl = {}\n",
    "        self.nextIndex = 0\n",
    "        \n",
    "    def load(self):\n",
    "        indices = sorted(self.file.iterIndices())\n",
    "        n = self.nextIndex = max(indices) + 1\n",
    "        for name, _ in dict.items(self.lists):\n",
    "            self.lists[name] = [None] * self.nextIndex\n",
    "        \n",
    "        for i in indices:\n",
    "            for name in self.lists:\n",
    "                if self.op[name].load:\n",
    "                    self.op[name].load(i)\n",
    "                \n",
    "        if self.key is not None:\n",
    "            self.assignKey(self.key, self.keyFunc)\n",
    "            \n",
    "    def i(self, key, client=True, dry=False):\n",
    "        keyVal = self.keyFunc(key, client=client)\n",
    "        \n",
    "        if keyVal not in self.tbl and not dry:\n",
    "            for name in self.lists:\n",
    "                assert len(self.lists[name]) == self.nextIndex, \"Internal error\"\n",
    "                self.lists[name].append(None)\n",
    "            self.tbl[keyVal] = self.nextIndex\n",
    "            self.nextIndex += 1\n",
    "        elif keyVal not in self.tbl and dry:\n",
    "            return None\n",
    "        \n",
    "        return self.tbl[keyVal]\n",
    "            \n",
    "    def save(self, keep=True, **kwargs):\n",
    "        assert ('i' in kwargs) ^ (self.key in kwargs), \"call to `save` must include exactly one of i= or the key name =\"\n",
    "        if 'i' in kwargs:\n",
    "            i = kwargs['i']\n",
    "            del kwargs['i']\n",
    "        else:\n",
    "            i = self.i(kwargs[self.key])\n",
    "        \n",
    "        for name in kwargs:\n",
    "            if name not in self.op:\n",
    "                raise AssertionError(f\"given save item '{name}' does not have data store ops initialized\")\n",
    "            if self.op[name].save:\n",
    "                self.op[name].save(kwargs[name], i=i, keep=keep)\n",
    "            \n",
    "    def get(self, name, **kwargs):\n",
    "        assert len(kwargs) == 1 and ('i' in kwargs or self.key in kwargs), \"must give assigned 'key' or i\"\n",
    "        if 'i' in kwargs:\n",
    "            i = kwargs['i']\n",
    "            del kwargs['i']\n",
    "        else:\n",
    "            i = self.i(kwargs[self.key], dry=True)\n",
    "        if i is None:\n",
    "            return None\n",
    "        return self.lists[name][i]\n",
    "        \n",
    "    ##################### INSTANCE BUILDING METHODS ##################### : \n",
    "    def assignKey(self, name, func):\n",
    "        assert name is not None, \"name to be used as key cannot be None\"\n",
    "        self.key = name\n",
    "        self.keyFunc = func\n",
    "        self.tbl = {self.keyFunc(x): i for i,x in enumerate(self.lists[self.key]) if x is not None}\n",
    "        \n",
    "    def assignOperations(self, name, *, load=I, save=I, keep=I, keepSave=None, keepClient=None):\n",
    "        if keepClient = None:\n",
    "            keepClient = keep\n",
    "            \n",
    "        class the(O()):\n",
    "            def load(*, i, keep=True):\n",
    "                x = load(self.file.io[name].read(i=i)) if load else None\n",
    "                if self.op[name].keep:\n",
    "                    self.lists[name][i] = self.op[name].keep(x)\n",
    "                return x\n",
    "            \n",
    "            def save(x, *, i, keep=True):\n",
    "                saved_x = save(x)\n",
    "                self.file.io[name].write(saved_x) if save else None\n",
    "                if self.op[name].keep:\n",
    "                    try:\n",
    "                        self.lists[name][i] = self.op[name].keep(saved_x, save=True)\n",
    "                    except (TypeError, AssertionError):\n",
    "                        self.lists[name][i] = self.op[name].keep(x, client=True)\n",
    "                \n",
    "            def keep(x, *, i, save=False, client=False):\n",
    "                assert not (save and client), \"only one of `save` and `client` can be specified\"\n",
    "                kept = None\n",
    "                if save:\n",
    "                    assert keepSave, \"`keepSave` must have been specifically given to use the `save=True` flag in `keep`\"\n",
    "                    kept = keepSave(x)\n",
    "                elif client:\n",
    "                    kept = keepClient(x)\n",
    "                else\n",
    "                    kept = keep(x)\n",
    "                if kept is not None:\n",
    "                    self.lists[name][i] = kept\n",
    "                return kept\n",
    "            \n",
    "        if not load:\n",
    "            the.load = False\n",
    "        if not save:\n",
    "            the.save = False\n",
    "        if not keep:\n",
    "            the.keep = False\n",
    "            \n",
    "        self.op[name] = the\n",
    "        if keep and name not in self.lists:\n",
    "            self.lists[name] = [None] * self.nextIndex\n",
    "    \n",
    "    \n",
    "@staticmethod\n",
    "def __IndexDataStore__from_specs(specs, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        specs[k] = v\n",
    "    f = IndexFileSystem(specs.dir, specs.key)\n",
    "    d = IndexDataStore(f, _factory_=True)\n",
    "    keyFuncClient = specs.keyFuncClient if 'keyFuncClient' in specs else specs.keyFunc\n",
    "    keyFunc = lambda keyVal, *, client=False: specs.keyFuncClient(keyVal) if client else specs.keyFunc(keyVal)\n",
    "    d.assignKey(specs.key, keyFunc)\n",
    "    _readwriteformat = {'read', 'write', 'format'}\n",
    "    _loadsavekeep = {'load', 'save', 'keep', 'keepSave', 'keepClient'}\n",
    "    for name, val in dict.items(specs):\n",
    "        f.assignIO(name, **{a: b for a,b in dict.items(val) if a in _readwriteformat})\n",
    "        d.assignOperations(name, **{a: b for a,b in dict.items(val) if a in _loadsavekeep})\n",
    "    d.load()\n",
    "IndexDataStore.from_specs = __IndexDataStore__from_specs\n",
    "\n",
    "\n",
    "class __IndexDataStore__SpecsHelper(metaclass=staticclass):\n",
    "    def json(op):\n",
    "        op.format = 'text'\n",
    "        op.read = json.loads\n",
    "        op.write = json.dumps\n",
    "        return op\n",
    "    def pickle(op):\n",
    "        op.format = 'bytes'\n",
    "        op.read = pickle.loads\n",
    "        op.write = pickle.dumps\n",
    "        return op\n",
    "IndexDataStore.SpecsHelper = __IndexDataStore__SpecsHelper\n",
    "IDSSH = IndexDataStore.SpecsHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelManager():\n",
    "    features_dir = '.features'\n",
    "    samples_dir = '.samples'\n",
    "    models_dir = 'models'\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        self.dir = directory if isinstance(directory, Path) else Path(directory)\n",
    "        self.dir.mkdir(exist_ok=True)\n",
    "        self.F = IndexDataStore.from_specs(FDS, dir=self.dir/self.features_dir)\n",
    "        self.S = IndexDataStore.from_specs(SDS, dir=self.dir/self.samples_dir)\n",
    "        (self.dir/self.models_dir).mkdir(exist_ok=True)\n",
    "        self.load()\n",
    "        \n",
    "    def load(self):\n",
    "        self.pms = {}\n",
    "        for fi, si in self.iterIndices():\n",
    "            self.make_params_manager(i=(fi, si))\n",
    "        \n",
    "    def iterIndices(self):\n",
    "        for fdir in (self.dir/self.models_dir).iterdir():\n",
    "            try:\n",
    "                fi = int(fdir.name)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            assert fdir.is_dir(), \"folder in models folder whose name is just a number must be a folder\"\n",
    "            assert fi < self.F.nextIndex, \"features folder found with greater index than I have labelled\"\n",
    "            for sdir in fdir.iterdir():\n",
    "                try:\n",
    "                    si = int(sdir.name)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                assert sdir.is_dir(), \"folder in models folder 1 layer down whose name is just a number must be a folder\"\n",
    "                assert si < self.S.nextIndex, \"samples folder found with greater index than I have labelled\"\n",
    "                yield (fi, si)\n",
    "        \n",
    "    def make_params_manager(self, features=None, samples=None, i=None):\n",
    "        assert features is not None and samples is not None or i is not None, \"invalid arguments to `make_param_manager`\"\n",
    "        if i is None:\n",
    "            fi, si = self.F.i(features), self.S.i(samples)\n",
    "        fi, si = i if i is not None else (fi, si)\n",
    "        pm = IndexDataStore.from_specs(PDS, dir=self.dir/self.models_dir/str(fi)/str(si))\n",
    "        self.pms[fi, si] = pm\n",
    "        return pm\n",
    "    \n",
    "    def i(features, samples): # purely convenience public interface\n",
    "        return self.F.i(features), self.S.i(samples)\n",
    "        \n",
    "    def PM(self, features, samples):\n",
    "        fi, si = self.F.i(features), self.S.i(samples)\n",
    "        if (fi, si) not in self.pms:\n",
    "            self.make_params_manager(i=(fi, si))\n",
    "        return self.pms[fi, si]\n",
    "        \n",
    "    def iPM(self, fi, si):\n",
    "        if (fi, si) not in self.pms:\n",
    "            self.make_params_manager(i=(fi, si))\n",
    "        return self.pms[fi, si]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPECS for managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgbNullDataset = lgb.Dataset(pd.DataFrame({'_a_': np.arange(100), '_b_': np.arange(100)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PDS(O()):\n",
    "    '''data transformation code inside `op` object:\n",
    "\n",
    "    client --save(+write)--> disk; client --keep--> memory; disk --load(+read)--> memory\n",
    "\n",
    "    read = text/bytes stream -> as-is object read from file\n",
    "    write = object to save as is to file -> text/bytes stream\n",
    "    load = as-is object read from file -> object to be loaded in memory\n",
    "    save = raw object given by client -> object to save as-is to file\n",
    "    keep = as-is object read from file -> object to keep in memory\n",
    "    keepClient = raw object given by client -> object to keep in memory\n",
    "    '''\n",
    "    \n",
    "\n",
    "    key = 'Params'\n",
    "    keyFunc = lambda params: tuple(sorted(dict.items(params)))\n",
    "\n",
    "    class op(O()):\n",
    "        @IDSSH.json\n",
    "        class Params(O()):\n",
    "            '''just a dict of the parameter value assignments'''\n",
    "            keepClient = dict\n",
    "            \n",
    "        @IDSSH.pickle\n",
    "        class Results(O()):\n",
    "            '''should be a dict-like of various things, most importantly including \"score\"'''\n",
    "            pass\n",
    "            \n",
    "        @IDSSH.pickle\n",
    "        class Training(O()):\n",
    "            '''tuple (aligning with samples training/cv split tuple) of LightGBM training eval DataFrames'''\n",
    "            keep = False\n",
    "            \n",
    "        @IDSSH.pickle\n",
    "        class Booster(O()):\n",
    "            '''the actual lgb.Booster model. well, a tuple of them, one for each cv set'''\n",
    "            #read = lambda file: lgb.Booster(model_file=file)\n",
    "            read = lambda x: tuple(lgb.Booster(train_set=lgbNullDataset).model_from_string(s, verbose=False) for s in x)\n",
    "            #write = lambda x, file: x.save_model(file)\n",
    "            write = lambda x: tuple(b.model_to_string() for b in x)\n",
    "            keep = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FDS(O()):\n",
    "    key = 'Feats'\n",
    "    keyFunc = lambda features: features\n",
    "    keyFuncClient = lambda features: frozenset(features)\n",
    "    class op(O()):\n",
    "        @IDSSH.json\n",
    "        class Feats(O()):\n",
    "            load = frozenset\n",
    "            save = sorted\n",
    "            keep = load\n",
    "    \n",
    "class SDS(O()):\n",
    "    key = 'Samps'\n",
    "    keyFunc = lambda samples: frozenset(frozenset(a) for a in samples)\n",
    "    class op(O()):\n",
    "        @IDSSH.json\n",
    "        class Samps(O()):\n",
    "            load = tuple\n",
    "            save = lambda samples: [sorted(a) for a in samples]\n",
    "            keepSave = tuple\n",
    "            keepClient = lambda samples: tuple(sorted(a) for a in samples)\n",
    "        @IDSSH.json\n",
    "        class Ctor(O()):\n",
    "            keepClient = lambda code: copy.deepcopy(dict(**code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~!~!~!~!~!~! Model (features/samples/parameters) Searching ~!~!~!~!~!~!~!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter search logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParamSearch():\n",
    "    mix = staticmethod(logavg)\n",
    "    \n",
    "    def __init__(self, specs):\n",
    "        self.specs = O.mycopy(specs)\n",
    "        self.setup_specs()\n",
    "        \n",
    "    def setup_specs(self):\n",
    "        dsc = self.specs.Discrete\n",
    "        dsc.keys = list(flatten( dsc.enc.keys() ))\n",
    "        dsc.assigns = [list(flatten(x)) for x in product(* dsc.enc.values() )]\n",
    "        obo = self.specs.OneByOne\n",
    "        for k,v in dict.items(obo.data):\n",
    "            for i,x in dict.items(obo.default):\n",
    "                if i not in v:\n",
    "                    v[i] = x\n",
    "    \n",
    "    def search(self):\n",
    "        dsc, obo = self.specs.Discrete, self.specs.OneByOne\n",
    "        for assign in dsc.assigns:\n",
    "            params = dict(zip(dsc.keys, assign))\n",
    "            coroutine = self.one_by_one()\n",
    "            for addon in coroutine:\n",
    "                params.update(addon)\n",
    "                coroutine.send((yield copy.deepcopy(params))); assert (yield) == None\n",
    "        \n",
    "        \n",
    "    def one_by_one(self):\n",
    "        obod = self.specs.OneByOne.data\n",
    "        \n",
    "        #! main algorithm ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        # initialize loop variables\n",
    "        params = {k: (v.b if 'b' in v else v.a[1]) for k,v in dict.items(obod)}\n",
    "        ranges = {k: v.a for k,v in dict.items(obod)}\n",
    "        scores = {k: [-np.inf, -np.inf] for k in params}\n",
    "        isdone = {k: False for k in params}\n",
    "        \n",
    "        # pre loop one-off work\n",
    "        base_score = yield params; assert (yield) == None\n",
    "        \n",
    "        # loop\n",
    "        for i in range(9999999999):\n",
    "            #! try new parameter values ############################## part A of loop work\n",
    "            \n",
    "            # initialize local loop variable\n",
    "            new_scores = {k: [-np.inf, -np.inf] for k in params}\n",
    "            new_params = {k: ( obod[k].cast(self.mix(v[0], v[1])),\n",
    "                               obod[k].cast(self.mix(v[1], v[2])) ) for k,v in dict.items(ranges)}\n",
    "            \n",
    "            # finish condition check\n",
    "            isdone = {k: v or i>=obod[k].lim for k,v in dict.items(isdone)}\n",
    "            if all(isdone.values()):\n",
    "                break\n",
    "                \n",
    "            # try new parameter values for all parameters\n",
    "            for key in list(params):\n",
    "                if i >= obod[key].lim:\n",
    "                    continue\n",
    "                orig = params[key]\n",
    "                params[key] = new_params[key][0]\n",
    "                scores[key][0] = yield params; assert (yield) == None\n",
    "                params[key] = new_params[key][1]\n",
    "                scores[key][1] = yield params; assert (yield) == None\n",
    "                params[key] = orig\n",
    "            \n",
    "            #! start setting up values for next loop ######################## part B of loop work\n",
    "            \n",
    "            # set params to the best found and see if it betters score, updating ranges also\n",
    "            #num_nochange = 0\n",
    "            for key in list(params):\n",
    "                if scores[key][0] > base_score and scores[key][0] >= scores[key][1]:\n",
    "                    params[key] = new_params[key][0]\n",
    "                    ranges[key] = [ranges[key][0], params[key], ranges[key][1]]\n",
    "                elif scores[key][1] > base_score and scores[key][1] >= scores[key][0]:\n",
    "                    params[key] = new_params[key][1]\n",
    "                    ranges[key] = [ranges[key][1], params[key], ranges[key][2]]\n",
    "                else:\n",
    "                    ranges[key] = [new_params[key][0], ranges[key][1], new_params[key][1]]\n",
    "                    #num_nochange += 1\n",
    "                \n",
    "            # send out new params\n",
    "            #if num_nochange < len(params):\n",
    "            base_score = yield params; assert (yield) == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specs for Search logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LPS(O()):\n",
    "    class Discrete(O()):\n",
    "        enc = {\n",
    "            ('max_depth','num_leaves'): #10\n",
    "                [(6,1<<6),(9,1<<7),(9,1<<9),(12,1<<8),(12,1<<10),(12,1<<12),(-1,1<<8),(-1,1<<10),(-1,1<<12),(-1,1<<14)],\n",
    "            #('max','num'): #10\n",
    "            #    [(6,6),(9,7),(9,9),(12,8),(12,10),(12,12),(-1,8),(-1,10),(-1,12),(-1,14)],\n",
    "        }\n",
    "        \n",
    "    class OneByOne(O()):\n",
    "        class info(O()):\n",
    "            a = \"main [a]rray data of 3 values, [min mid max]\"\n",
    "            b =  \"[b]ack up value, i.e. default value if array doesn't give better results\"\n",
    "            cast = \"function to apply to values before using to cast to the right dtype\"\n",
    "            lim = \"maximum number of iterations of searching in this hyperparameter\"\n",
    "        class default(O()):\n",
    "            cast = keepSigFig(2)\n",
    "            lim = 2\n",
    "        data = {\n",
    "            'min_data_in_leaf': O(a=[1,60,375], cast=round),\n",
    "            'min_sum_hessian_in_leaf': O(a=[0,50,200]),\n",
    "            'lambda_l1': O(a=[0,.02,.2], b=0, lim=1),\n",
    "            'lambda_l2': O(a=[0,.02,.2], b=0, lim=1),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features / Samples(train/cv split) search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KaggleMetric():\n",
    "    def __init__(self, incr=0):\n",
    "        self.incr = incr\n",
    "    \n",
    "    def attach(self, ms):\n",
    "        L, s = ms._L, ms._s\n",
    "        for Ltr, Lcv, tr, cv in zip(L.tr, L.cv, s.tr, s.cv):\n",
    "            Ltr.timeFactor = ms.Y.time[tr].factorize()[0]\n",
    "            Lcv.timeFactor = ms.Y.time[cv].factorize()[0]\n",
    "            Ltr.value = (ms.Y.upDown1*ms.Y.absVal1)[tr]\n",
    "            Lcv.value = (ms.Y.upDown1*ms.Y.absVal1)[cv]\n",
    "            Ltr.i = 0\n",
    "            Lcv.i = 0\n",
    "    \n",
    "    def __call__(self, preds, valid_data):\n",
    "        df_time = valid_data.timeFactor\n",
    "        #labels = valid_data.get_label()\n",
    "        values = valid_data.value\n",
    "        #assert len(labels) == len(df_time)\n",
    "\n",
    "        preds = preds*2-1\n",
    "        #labels = labels*2-1\n",
    "        x_t = preds * values\n",
    "\n",
    "        # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n",
    "        # is a pd.Series and call `group_by`\n",
    "        x_t_sum = x_t.groupby(df_time).sum()\n",
    "        score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "        valid_data.i += self.incr\n",
    "        return 'kaggle', score+valid_data.i, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMS(O()):\n",
    "    model = O(\n",
    "        time = 'time',\n",
    "        value = 'y',\n",
    "        target = 'target',\n",
    "        weight = 'weight',\n",
    "    )\n",
    "    \n",
    "    metrics = [KaggleMetric()]\n",
    "    search = LPS\n",
    "    \n",
    "    class Features(O()):\n",
    "        '''features selection groups'''\n",
    "        data = [\n",
    "            ['f1','f2','f3'],\n",
    "            ['f4','f5','f6'],\n",
    "        ]\n",
    "    \n",
    "    class Samples(O()):\n",
    "        '''sample learning/cv split'''\n",
    "        enc = [\n",
    "            O(method='GroupShuffleSplit.2', kwargs=dict(n_splits=1, test_size=.5, random_state=44), groups='quarter'),\n",
    "        ]\n",
    "        \n",
    "    class Params(O()):\n",
    "        '''parameters constant settings'''\n",
    "        data = dict(\n",
    "            objective = 'binary',\n",
    "            num_iterations = 10000,\n",
    "            early_stopping_round = 50,\n",
    "            learning_rate = .05,\n",
    "            seed = 44,\n",
    "            bagging_seed = 45,\n",
    "            feature_fraction_seed = 46,\n",
    "        )\n",
    "        \n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "        \n",
    "class ModelSearch():\n",
    "    \n",
    "    def __init__(self, specs, mm, *, X, Y):\n",
    "        self.specs = specs\n",
    "        self.mm = mm\n",
    "        self.setup_specs()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def setup_specs(self):\n",
    "        self.specs.Samples.data = []\n",
    "        for code in self.specs.Samples.enc:\n",
    "            if code.method == 'GroupShuffleSplit.2':\n",
    "                tr, cv = next(GroupShuffleSplit(**code.kwargs).split(self.X, self.Y, groups=Y[code.groups]))\n",
    "                self.specs.Samples.data += [((tr, cv), (cv, tr))]\n",
    "            elif code.method == 'GroupKFold':\n",
    "                self.specs.Samples.data.append(list(GroupKFold(**code.kwargs).split(self.X, self.Y, groups=Y[code.groups])))\n",
    "            else:\n",
    "                assert False, f'sampling method \"{code.method}\" not implemented'\n",
    "        \n",
    "    def run(self):\n",
    "        for feats, (samps, sampsEnc) in product(self.specs.Features.data,\n",
    "                                                zip(self.specs.Samples.data, self.specs.Samples.enc)):\n",
    "            self.setup_training(feats, samps)\n",
    "            pm = self.mm.PM(feats, samps)\n",
    "            self.mm.S.save(Samps=samps, Ctor=sampsEnc)\n",
    "            search = ParamSearch(self.specs.search)\n",
    "            loop = search.search()\n",
    "            for params in loop:\n",
    "                params = dict(**self.specs.Params.data, **params)\n",
    "                self.train(params) # sets some state attributes in self: self._save\n",
    "                pm.save(**self._save)\n",
    "                loop.send(self._save.Results['score'])\n",
    "                del self._save\n",
    "                \n",
    "    def setup_training(self, feats, samps):\n",
    "        self._feats, self._samps = feats, samps\n",
    "        _X = self.X[feats]\n",
    "        _dummy = pd.Series(range(len(_X)),index=_X.index)\n",
    "        _s = O()\n",
    "        _s.tr, _s.cv = tuple(_dummy.isin(s[0]) for s in samps), tuple(_dummy.isin(s[1]) for s in samps)\n",
    "        lgb_data_info = dict(\n",
    "            feature_name = list(_X.columns),\n",
    "            categorical_feature = list(_X.dtypes[_X.dtypes.isin([np.int64,np.int32])].index),\n",
    "            free_raw_data = False,\n",
    "        )\n",
    "        _L = self._L = O()\n",
    "        _L.tr = [lgb.Dataset(_X[tr], P[self.specs.model.target][tr], **lgb_data_info,\n",
    "                            **({'weight': self.specs.model.weight} if 'weight' in self.specs.model else {}))\n",
    "                for tr in _s.tr]\n",
    "        _L.cv = [lgb.Dataset(_X[cv], P[self.specs.model.target][cv], reference=Ltr, **lgb_data_info,\n",
    "                            **({'weight': self.specs.model.weight} if 'weight' in self.specs.model else {}))\n",
    "                for cv, Ltr in zip(_s.cv, _L.tr)]\n",
    "        for m in self.specs.metrics:\n",
    "            if hasattr(m, 'attach'):\n",
    "                m.attach(self)\n",
    "        #TODO implement both logloss and Kaggle metric, and stop only when both don't improve in whatever num rounds\n",
    "        \n",
    "                \n",
    "    def train(self, params):\n",
    "        def iter_samples():\n",
    "            evals_result = {}\n",
    "            bst = lgb.train(params, Ltr, valid_sets=[Ltr,Lcv], valid_names=['tr','cv'],\n",
    "                      feval=lgb_kaggle_metric, evals_result=evals_result, verbose_eval=False) #TODO lgb_kaggle_metric\n",
    "            df_results = (pd.DataFrame(evals_result['tr']), pd.DataFrame(evals_results['cv']))\n",
    "            yield bst, df_results\n",
    "        bsts, dfs = list(zip(**iter_samples()))\n",
    "        #TODO make self._save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Run Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -.-.-.-.-.-.-.-.-.-.-.-.-.- old shit / testing -.-.-.-.-.-.-.-.-.-.-.-.-.-.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'What' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-dc92f66a5036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mYo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mWhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mderp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-dc92f66a5036>\u001b[0m in \u001b[0;36mYo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mWhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mderp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mclass\u001b[0m \u001b[0mAnother\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0myep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mYo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-dc92f66a5036>\u001b[0m in \u001b[0;36mAnother\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mderp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mAnother\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0myep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mYo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'What' is not defined"
     ]
    }
   ],
   "source": [
    "# PUREL TESTING\n",
    "class HI(O()):\n",
    "    class DERP():\n",
    "        val = -99\n",
    "        dir = Path('.')\n",
    "        def yo(x):\n",
    "            def y(a):\n",
    "                return x(a) + 1000\n",
    "            return y\n",
    "        my = O(what=yo(lambda a: a ** 2))\n",
    "        hm = O(func=lambda x: x * __class__.val, goto=lambda x: __class__.dir/x)\n",
    "        class WHAT(O()):\n",
    "            dude = lambda x: x + 10\n",
    "        class YES(O()):\n",
    "            dude = WHAT.dude\n",
    "HI.DERP.my.what(5)\n",
    "HI.DERP.hm.func(5)\n",
    "HI.DERP.dir = Path('what')\n",
    "HI.DERP.hm.goto('.git')\n",
    "HI.DERP.WHAT.dude(0)\n",
    "\n",
    "# ANOTHER PURE TESTING\n",
    "def dothe(self):\n",
    "    self.val = 4\n",
    "    top = 100\n",
    "    class Derp(O()):\n",
    "        def func(x):\n",
    "            return x + self.val\n",
    "        def top(y):\n",
    "            return y * top\n",
    "    self.derp = Derp\n",
    "item = Stop()\n",
    "dothe(item)\n",
    "item.derp.top(3)\n",
    "\n",
    "class Yo(O()):\n",
    "    class What(O()):\n",
    "        derp = 5\n",
    "    class Another(O()):\n",
    "        yep = What.derp\n",
    "Yo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
