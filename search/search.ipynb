{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "from search.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_stack_trans(x):\n",
    "    bot = x.min()\n",
    "    assert bot<0\n",
    "    return np.exp((x-bot)/bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter search logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParamSearch():\n",
    "    mix = staticmethod(logavg)\n",
    "    \n",
    "    def __init__(self, specs):\n",
    "        self.specs = O.mycopy(specs)\n",
    "        self.setup_discrete()\n",
    "        \n",
    "    def setup_discrete(self):\n",
    "        dsc = self.specs.Discrete\n",
    "        dsc.keys = list(flatten( dsc.enc.keys() ))\n",
    "        dsc.assigns = [list(flatten(x)) for x in product(* dsc.enc.values() )]\n",
    "        \n",
    "    def setup_onebyone(self, label):\n",
    "        obo, oboe = self.specs.OneByOne, self.specs.OneByOne.enc[label]\n",
    "        \n",
    "        if not isinstance(obo.base, Iterable):\n",
    "            obo.base = [obo.base]\n",
    "        \n",
    "        data = obo.data = {}\n",
    "        for k,v in dict.items(oboe):\n",
    "            u = data[k] = O(**v)\n",
    "            for i,x in dict.items(obo.default):\n",
    "                if i not in u:\n",
    "                    u[i] = x\n",
    "                elif isinstance(u[i], LambdaType):\n",
    "                    u[i] = u[i](x)\n",
    "    \n",
    "    def search(self):\n",
    "        self._best = -float('inf')\n",
    "        dsc, obo = self.specs.Discrete, self.specs.OneByOne\n",
    "        for di, assign in enumerate(dsc.assigns):\n",
    "            assert 0 not in dsc.stop\n",
    "            if di in dsc.stop and dsc.stop[di] > self._best:\n",
    "                break\n",
    "            if di in dsc.obo:\n",
    "                self.setup_onebyone(dsc.obo[di])\n",
    "            params = dict(zip(dsc.keys, assign))\n",
    "            coroutine = self.one_by_one()\n",
    "            for addon in coroutine:\n",
    "                SEE.queue({k:v for k,v in dict.items(params) if k not in addon})\n",
    "                SEE.queue(addon)\n",
    "                params.update(addon)\n",
    "                results = (yield copy.deepcopy(params))\n",
    "                self._best = max(results['score'], self._best)\n",
    "                coroutine.send(results); assert (yield) is None\n",
    "        \n",
    "        \n",
    "    def one_by_one(self):\n",
    "        obo, obod = self.specs.OneByOne, self.specs.OneByOne.data\n",
    "        \n",
    "        #! main algorithm ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        # initialize loop variables\n",
    "        params = {k: (v.b if 'b' in v else v.a[1]) for k,v in dict.items(obod)}\n",
    "        ranges = {k: v.a for k,v in dict.items(obod)}\n",
    "        scores = {k: [-np.inf, -np.inf] for k in params}\n",
    "        isdone = {k: False for k in params}\n",
    "        \n",
    "        # pre loop one-off work\n",
    "        base_score = (yield params)['score']; assert (yield) is None\n",
    "        \n",
    "        # loop\n",
    "        for i in range(999_999_999):\n",
    "            cutoff = obo.base[i] if i<len(obo.base) else obo.base[-1]\n",
    "            if base_score <= cutoff:\n",
    "                return\n",
    "            \n",
    "            #! try new parameter values ############################## part A of loop work\n",
    "            \n",
    "            # initialize local loop variable\n",
    "            new_scores = {k: [-np.inf, -np.inf] for k in params}\n",
    "            new_params = {k: ( obod[k].cast(self.mix(v[0], v[1])),\n",
    "                               obod[k].cast(self.mix(v[1], v[2])) ) for k,v in dict.items(ranges)}\n",
    "            \n",
    "            # finish condition check\n",
    "            isdone = {k: v or i>=obod[k].lim for k,v in dict.items(isdone)}\n",
    "            if all(isdone.values()):\n",
    "                break\n",
    "                \n",
    "            # try new parameter values for all parameters\n",
    "            for key in list(params):\n",
    "                if i >= obod[key].lim:\n",
    "                    continue\n",
    "                orig = params[key]\n",
    "                params[key] = new_params[key][0]\n",
    "                scores[key][0] = (yield params)['score']; assert (yield) is None\n",
    "                params[key] = new_params[key][1]\n",
    "                scores[key][1] = (yield params)['score']; assert (yield) is None\n",
    "                params[key] = orig\n",
    "            \n",
    "            #! start setting up values for next loop ######################## part B of loop work\n",
    "            \n",
    "            # set params to the best found and see if it betters score, updating ranges also\n",
    "            #CODE num_nochange = 0\n",
    "            for key in list(params):\n",
    "                if scores[key][0] > base_score and scores[key][0] >= scores[key][1]:\n",
    "                    params[key] = new_params[key][0]\n",
    "                    ranges[key] = [ranges[key][0], params[key], ranges[key][1]]\n",
    "                elif scores[key][1] > base_score and scores[key][1] >= scores[key][0]:\n",
    "                    params[key] = new_params[key][1]\n",
    "                    ranges[key] = [ranges[key][1], params[key], ranges[key][2]]\n",
    "                else:\n",
    "                    ranges[key] = [new_params[key][0], ranges[key][1], new_params[key][1]]\n",
    "                    #num_nochange += 1\n",
    "                \n",
    "            # send out new params\n",
    "            #CODE if num_nochange < len(params):\n",
    "            base_score = (yield params)['score']; assert (yield) is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features / Samples(train/cv split) search logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model searching logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelSearch():\n",
    "    \n",
    "    def __init__(self, specs, mm, *, log=None, tag=None, tags=None, verbose_eval=None):\n",
    "        self.specs = specs\n",
    "        self.log = log\n",
    "        global SEE #TODO unfortunate very hack, this is just to get ParamSearch to print in a way consistent with\n",
    "        SEE = log.print # previously done search logs, i.e. have a logger that prints on a level higher than self.log\n",
    "        assert tag is None or tags is None\n",
    "        if tag is None and tags is None:\n",
    "            tags = []\n",
    "        elif tags is None:\n",
    "            tags = [tag]\n",
    "        self.tags = frzset(tags)\n",
    "        self.verbose_eval = verbose_eval\n",
    "        self.mm = mm\n",
    "        self._DBG_ = O()\n",
    "        if 'weight_stack' in self.specs.model and self.specs.model.weight_stack:\n",
    "            self.e = pd.read_pickle('/big/data/saves/e-amour-tt7.pickle')\n",
    "#         self.setup_specs()\n",
    "        \n",
    "#     def setup_specs(self):\n",
    "#         for si in range(self.mm.S.nextIndex):\n",
    "#             ctor = self.mm.S.get('Ctor', i=si)\n",
    "#             samps = self.specs.Samples.ctor2samps(ctor)\n",
    "#             self.mm.S.save(Samps=samps, i=si)\n",
    "        \n",
    "#     def setup_specs(self):\n",
    "#         self.specs.Samples.data = []\n",
    "#         for code in self.specs.Samples.enc:\n",
    "#             if code.method == 'group.2':\n",
    "#                 tr, cv = set(code.data[0]), set(code.data[1])\n",
    "#                 tr, cv = self.Y[code.groups].isin(tr), self.Y[code.groups].isin(cv)\n",
    "#                 tr, cv = self.Y.index[tr], self.Y.index[cv]\n",
    "#                 self.specs.Samples.data += [((tr, cv), (cv, tr))]\n",
    "#             elif code.method == 'GroupShuffleSplit.2':\n",
    "#                 tr, cv = next(GroupShuffleSplit(**code.kwargs).split(self.X, self.Y, groups=self.Y[code.groups]))\n",
    "#                 tr, cv = self.Y.index[tr], self.Y.index[cv]\n",
    "#                 self.specs.Samples.data += [((tr, cv), (cv, tr))]\n",
    "# #             elif code.method == 'GroupKFold':\n",
    "# #                 self.specs.Samples.data.append(tuple(GroupKFold(**code.kwargs)\n",
    "# #                                                      .split(self.X, self.Y, groups=self.Y[code.groups])))\n",
    "#             else:\n",
    "#                 assert False, f'sampling method \"{code.method}\" not implemented'\n",
    "        \n",
    "    def iter_feats_samps(self, t=0):\n",
    "        t = t-1\n",
    "        while True:\n",
    "            t = self.specs.Step.func(self._best[1].Results if hasattr(self, '_best') else None, t=t)\n",
    "            self.log(f't = {t}', '='*99)\n",
    "            if t is None:\n",
    "                break\n",
    "            \n",
    "            featsObj = self.specs.Features.func(t=t)\n",
    "            sampsObj = self.specs.Samples.func(t=t)\n",
    "            if featsObj is None or sampsObj is None:\n",
    "                break\n",
    "            fi = self.mm.F.save(**featsObj)\n",
    "            si = self.mm.S.save(**sampsObj)\n",
    "            self._DBG_.fs = featsObj, sampsObj\n",
    "            \n",
    "            self.X, self.Y = self.specs.Data.func(t=t)\n",
    "            assert (self.X.index == self.Y.index).all()\n",
    "            \n",
    "            yield (fi, featsObj['Feats']), (si, sampsObj['Samps'])\n",
    "        \n",
    "    def walk(self, t=0):\n",
    "        for (fi, feats), (si, samps) in self.iter_feats_samps(t):\n",
    "            best_score = -np.inf # this best score here is on a per (feats,samps) basis, maxed over params\n",
    "            pm = self.mm.iPM(fi, si)\n",
    "            self._pm = pm\n",
    "            \n",
    "            self.ho = self.specs.Holdout.func(t=t)\n",
    "            if not isinstance(self.ho, list): #NEW k-fold flow\n",
    "                self.ho = [self.ho]\n",
    "            self.setup_training(feats, samps, i=(fi,si))\n",
    "            search = ParamSearch(self.specs.search)\n",
    "            loop = search.search()\n",
    "            for k, params in enumerate(loop): # params are deepcopied out, so can safely save them as-is!\n",
    "                self.log(f' k = {k}', '-'*99)\n",
    "                paramsUse = dict(**self.specs.Params.data, **params)\n",
    "                \n",
    "                try:\n",
    "                    del params['num_iterations'] # num_iterations too finicky to save\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                \n",
    "                pi = pm.i(params)\n",
    "                results = pm.get('Results', i=pi)\n",
    "                if results:\n",
    "                    if results['score'] > best_score:\n",
    "                        self._save = O(Results=results)\n",
    "                        self._best = (dict(**params), O(**self._save)) #TODO I still copy params here to be safe (needed)?\n",
    "                else:\n",
    "                    self.log('training...')\n",
    "                    self.train(paramsUse) # sets some state attributes in self: self._save\n",
    "                    results = self._save.Results\n",
    "                    if results['score'] > best_score:\n",
    "                        self._best = (dict(**params), O(**self._save)) #TODO I still copy params here to be safe (needed)?\n",
    "                    del self._save.Boosters#, self._save.Training\n",
    "                    already_tags = pm.get('Tags', i=pi)\n",
    "                    already_tags = already_tags if already_tags is not None else frozenset()\n",
    "                    pm.save(i=pi, Tags=already_tags|self.tags, **self._save)\n",
    "                loop.send(results)\n",
    "                yield (fi, si), results\n",
    "                #del self._save #TODO probably wanna uncomment this in production\n",
    "            #pm.save(Params=self._best[0], **self._best[1]) #TODO forget saving the best model\n",
    "            \n",
    "    def run(self, t=0):\n",
    "        for _ in self.walk(t):\n",
    "            pass\n",
    "        \n",
    "    def setup_training(self, feats, samps, *, i=None):\n",
    "        SEE('setup_training', '%'*44)\n",
    "        SEE(f'len(feats) = {len(feats)}')\n",
    "        SEE(self._DBG_.fs[0]['Ctor'] if 'Ctor' in self._DBG_.fs[0] else self._DBG_.fs[0]['Feats'])\n",
    "        SEE(f'len(samps) = {len(samps)}')\n",
    "        SEE(self._DBG_.fs[1]['Ctor'] if 'Ctor' in self._DBG_.fs[1] else self._DBG_.fs[1]['Samps'])\n",
    "#         samps = self.mm.S.get('Samps', i=i[1]) # this `samps` is a tuple of tuple of frozensets\n",
    "        if hasattr(self, '_X'):\n",
    "            delattr(self, '_X')\n",
    "        if list(self.X.columns) != list(feats):\n",
    "            _X = self._X = self.X[feats]\n",
    "        else:\n",
    "            _X = self._X = self.X\n",
    "        #_dummy = pd.Series(range(len(_X)), index=_X.index)\n",
    "        _s = self._s = O()\n",
    "        _s.tr, _s.cv = tuple(_X.index.isin(s[0]) for s in samps), tuple(_X.index.isin(s[1]) for s in samps)\n",
    "        lgb_data_info = dict(\n",
    "            feature_name = list(_X.columns),\n",
    "            categorical_feature = list(_X.dtypes[_X.dtypes.isin([np.int64,np.int32])].index),\n",
    "            free_raw_data = False,\n",
    "        )\n",
    "        _L = self._L = O()\n",
    "#         #DEBUG: #NOPE\n",
    "#         ctor = self._DBG_.fs[0]['Ctor']\n",
    "#         name = ctor[ctor.index('<')+1:ctor.index('>')]\n",
    "#         pd.to_pickle((_X, self.Y, lgb_data_info, self.specs.model.target, self.specs.model.weight, _s.tr, _s.cv),\n",
    "#                      f'/big/data/fuck/{name}.pkl')\n",
    "#         #:DEBUG #NOPE\n",
    "\n",
    "        def _weight(ds):\n",
    "            if 'weight_stack' not in self.specs.model or not self.specs.model.weight_stack:\n",
    "                return self.Y[self.specs.model.weight][ds]\n",
    "            else:\n",
    "                name = dict(flat_weight='a',vp10_weight='m')[self.specs.model.weight] + self.specs.model.name\n",
    "                print('_weight', name)\n",
    "                return self.Y[self.specs.model.weight][ds] * weight_stack_trans(self.Y.time[ds].map(self.e[name]))\n",
    "        _L.tr = [lgb.Dataset(_X[tr], self.Y[self.specs.model.target][tr], **lgb_data_info,\n",
    "                            **({'weight': _weight(tr)} if 'weight' in self.specs.model else {}))\n",
    "                for tr in _s.tr]\n",
    "        _L.cv = [lgb.Dataset(_X[cv], self.Y[self.specs.model.target][cv], reference=Ltr, **lgb_data_info,\n",
    "                            **({'weight': _weight(cv)} if 'weight' in self.specs.model else {}))\n",
    "                for cv, Ltr in zip(_s.cv, _L.tr)]\n",
    "        self.Lho = [lgb.Dataset(_X[h], self.Y[self.specs.model.target][h], **lgb_data_info,\n",
    "                            **({'weight': _weight(h)} if 'weight' in self.specs.model else {}))\n",
    "                for h in self.ho] #NEW k-fold flow\n",
    "        m = self.specs.metric\n",
    "        if hasattr(m, 'attach'):\n",
    "            m.attach(self)\n",
    "        #TODO implement both logloss and Kaggle metric, and stop only when both don't improve in whatever num rounds\n",
    "    \n",
    "    def train(self, params):\n",
    "        def iter_samples():\n",
    "            Lho = self.Lho\n",
    "            ho_names = ['ho'+str(i) for i in range(len(self.ho))]\n",
    "            for Ltr, Lcv in zip(self._L.tr, self._L.cv):\n",
    "                evals_result = {}\n",
    "                # fucking LightGBM deletes 'num_iterations' from `params` after training, like WTF???\n",
    "                def _temp():\n",
    "                    self._gbm = lgb.train(dict(params), Ltr, valid_sets=[Ltr,Lcv]+Lho, valid_names=['tr','cv']+ho_names,\n",
    "                        feval=self.specs.metric, evals_result=evals_result, verbose_eval=self.verbose_eval)\n",
    "                if hasattr(self, '_gbm'):\n",
    "                    del self._gbm\n",
    "                %time _temp()\n",
    "                bst = self._gbm\n",
    "                \n",
    "                df_results = (\n",
    "                    (pd.DataFrame(evals_result['tr']), pd.DataFrame(evals_result['cv'])) +\n",
    "                    tuple(pd.DataFrame(evals_result['ho'+str(i)]) for i in range(len(self.ho)))\n",
    "                )\n",
    "                yield bst, df_results\n",
    "        bsts, dfs = zip(*iter_samples())\n",
    "        \n",
    "        # find best nboost\n",
    "        _ms = self.specs.metric.score\n",
    "        _i, _sco = None, -np.inf\n",
    "        for i in dfs[0][1].index: # 1 for cv (0 for tr, and 2+ for ho)\n",
    "            sco = self.specs.agg_metric(dfs, 1, i, _ms)\n",
    "            if sco > _sco:\n",
    "                _i = i\n",
    "                _sco = sco\n",
    "        \n",
    "#         # find best nboost # use new flow with multiple transformed scores and take the best one\n",
    "#         wait = self.specs.early_stopping\n",
    "#         _i, _ms, _sco = None, None, -np.inf\n",
    "#         ct = 0\n",
    "#         for i in dfs[0][1].index: # in second layer of index, 1 for cv (0 for tr, and 2+ for ho)\n",
    "#             better = False\n",
    "#             for metric in dfs[0][1].columns:\n",
    "#                 sco = self.specs.agg_metric(dfs, 1, i, metric)\n",
    "#                 if sco > _sco:\n",
    "#                     _sco = sco\n",
    "#                     _ms = metric\n",
    "#                     better = True\n",
    "#             if better:\n",
    "#                 _i = i\n",
    "#                 ct = 0\n",
    "#             else:\n",
    "#                 ct += 1\n",
    "#                 if ct > wait:\n",
    "#                     _i = i-ct\n",
    "#                     break\n",
    "        \n",
    "        class save(O()):\n",
    "            Training = dfs\n",
    "            Boosters = bsts\n",
    "            class Results(O()):\n",
    "                nbest = _i+1\n",
    "                train = tuple(dft[0].loc[_i,_ms] for dft in dfs)\n",
    "                scores = tuple(dft[1].loc[_i,_ms] for dft in dfs)\n",
    "                score = self.specs.agg_metric(dfs, 1, _i, _ms)\n",
    "                holdout = tuple(tuple(dft[j].loc[_i,_ms] for j in range(2,len(dft))) for dft in dfs)\n",
    "        \n",
    "        # new addition, hook to save actual answers\n",
    "        def iter_ans():\n",
    "            for bst, tr, cv, Ltr, Lcv in zip(bsts, self._s.tr, self._s.cv, self._L.tr, self._L.cv):\n",
    "                assetCodes = ['assetCodeId'] if 'assetCodeId' in self.Y else ['__0__assetCodeId','__1__assetCodeId']\n",
    "                ans = []\n",
    "                _Y = self.Y[['time']+assetCodes+[self.specs.model.target,self.specs.model.weight]][cv]\n",
    "                _Y['guess'] = bst.predict(self._X[cv], num_iteration=_i+1)*2-1\n",
    "                ans.append(_Y)\n",
    "                for ho in self.ho:\n",
    "                    _Y = self.Y[['time']+assetCodes+[self.specs.model.target,self.specs.model.weight]][ho]\n",
    "                    _Y['guess'] = bst.predict(self._X[ho], num_iteration=_i+1)*2-1\n",
    "                    ans.append(_Y)\n",
    "                yield ans\n",
    "        save.Answers = list(iter_ans())\n",
    "        # end new addition\n",
    "        \n",
    "        self._save = save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
